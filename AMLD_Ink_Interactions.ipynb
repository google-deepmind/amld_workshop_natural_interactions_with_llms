{"cells":[{"cell_type":"markdown","metadata":{"id":"dgx4QQ_Mq7AT"},"source":["# Setting up code repositories and datasets.\n","\n","This section imports all the code and downloads all the data required for the workshop. You can explore the assets used by this notebook by clicking on the navigation bar on the left. All datasets are located under `data/` while model files are located under `models/`. This will take a few minutes ⏳!\n","\n","**Important note**: you may need to run `!gcloud init` in a cell the first time you use the `gcloud` command."]},{"cell_type":"markdown","metadata":{"id":"F7YYSOL58CbE"},"source":["## Code setup\n","Import code and clone repositories required for the workshop."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cwx-pBVsPvet"},"outputs":[],"source":["import copy\n","import functools\n","import glob\n","import io\n","import json\n","import os\n","import random\n","import shutil\n","import sys\n","import warnings\n","\n","from IPython.display import HTML, Javascript, display\n","from PIL import Image, ImageDraw\n","import tensorflow as tf\n","from tqdm import tqdm\n","import numpy as np\n","\n","# The T4 runtime is tight on memory to finetune this model. Preallocate\n","# all memory ahead of time to avoid OOM'ing due to fragmentation.\n","os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WNZGsLE0G9O"},"outputs":[],"source":["!git clone https://github.com/google-deepmind/amld_workshop_natural_interactions_with_llms.git\n","!git clone --branch=main --depth=1 https://github.com/google-research/big_vision big_vision_repository\n","\n","# Install missing dependencies. Assume jax~=0.4.25 with GPU available.\n","!pip3 install -q \"crcmod\" \"overrides\" \"ml_collections\" \"einops~=0.7\" \"sentencepiece\" \"jiwer\"\n","\n","sys.path.append('amld_workshop_natural_interactions_with_llms')\n","sys.path.append(\"big_vision_repository\")\n","\n","import arrow_ink_tools\n","import data_processing\n","import document_editing\n","import in_context_learning\n","import metrics\n","import model_api\n","import notebook_canvas\n","import paligemma_gesture_preparation\n","import paligemma_tools\n","import rendering\n","import sampling\n","\n","import ml_collections\n","import sentencepiece\n","\n","import big_vision\n","import jax\n","import big_vision.models.vit\n","from big_vision.models.proj.paligemma import gemma_bv\n","from big_vision.models.proj.paligemma import paligemma\n","from big_vision.trainers.proj.paligemma import predict_fns\n","import big_vision.datasets.jsonl\n","import big_vision.utils\n","import big_vision.sharding\n","import jax.numpy as jnp"]},{"cell_type":"markdown","metadata":{"id":"JwMKHCEK8GPI"},"source":["## Data setup\n","This will download all the workshop data to the local filesystem."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3KQ45tpiuwwx"},"outputs":[],"source":["INK_GCP_BUCKET_URL = 'gs://amld_workshop_natural_interactions_with_llms'\n","BIG_VISION_BUCKET_URL = 'gs://big_vision'\n","\n","PAGES_DIR = 'data/wikipedia_public'\n","INK_VALID_PATH = 'data/gestures/ink_gestures_valid.json'\n","INK_TRAIN_PATH = 'data/gestures/ink_gestures_train.json'\n","TF_VALID_PATH = 'data/gestures/annotation_gestures_valid.tfrecord'\n","TF_TRAIN_PATH = 'data/gestures/annotation_gestures_train.tfrecord'\n","PALIGEMMA_MODEL_PATH = 'models/paligemma_ft_digink_448.b16.npz'\n","PALIGEMMA_TOKENIZER_PATH = 'models/paligemma_tokenizer.model'\n","\n","\n","def download_workshop_data(file_path: str, bucket=INK_GCP_BUCKET_URL):\n","  file_name = os.path.basename(file_path)\n","  if os.path.exists(file_path):\n","    return\n","\n","  print(f'Downloading {file_name} into {file_path}.')\n","  if file_name.endswith('.zip'):\n","    !gsutil cp {os.path.join(bucket, file_name)} {file_path}\n","    !unzip -q {file_path} -d {os.path.dirname(file_path)}\n","  else:\n","    !gsutil cp {os.path.join(bucket, file_name)} {file_path}\n","  print()\n","\n","download_workshop_data(os.path.join(PAGES_DIR, 'wikipedia_public.zip'))\n","download_workshop_data(INK_TRAIN_PATH)\n","download_workshop_data(INK_VALID_PATH)\n","download_workshop_data(TF_TRAIN_PATH)\n","download_workshop_data(TF_VALID_PATH)\n","download_workshop_data(PALIGEMMA_MODEL_PATH)\n","download_workshop_data(PALIGEMMA_TOKENIZER_PATH, bucket=BIG_VISION_BUCKET_URL)"]},{"cell_type":"markdown","metadata":{"id":"Kz6gaNiD6Wmg"},"source":["## Utility functions\n","This section defines additional utility functions to load, manipulate and visualize the data that we just downloaded and that we will be using during the workshop."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3QkNTjfXPpf"},"outputs":[],"source":["_IMAGE_RESOLUTION = 448\n","\n","def convert_bbox(\n","    left: float, top: float, right: float, bottom: float\n",") -> document_editing.BoundingBox:\n","  \"\"\"Convert a bounding box from a list of integers to a dataclass instance.\"\"\"\n","  return document_editing.BoundingBox(\n","      top=top,\n","      left=left,\n","      bottom=bottom,\n","      right=right,\n","  )\n","\n","_EMPTY_STRING = tf.convert_to_tensor(\"\", dtype=\"string\").numpy()\n","_EMPTY_BOUNDING_BOX = np.array([0.0, 0.0, 0.0, 0.0], dtype=\"float32\")\n","_EMPTY_IMAGE = tf.io.encode_png(tf.constant([[[0]]], dtype=\"uint8\")).numpy()\n","FEATURE_SPEC = {\n","    \"ink_hash\": tf.io.FixedLenFeature(\n","        [], dtype=tf.string, default_value=_EMPTY_STRING\n","    ),\n","    \"example_id\": tf.io.FixedLenFeature(\n","        [], dtype=tf.string, default_value=_EMPTY_STRING\n","    ),\n","    \"annotation_bbox\": tf.io.FixedLenFeature(\n","        [4], dtype=tf.float32, default_value=_EMPTY_BOUNDING_BOX\n","    ),\n","    \"composition_bbox\":  tf.io.FixedLenFeature(\n","        [4], dtype=tf.float32, default_value=_EMPTY_BOUNDING_BOX\n","    ),\n","    \"label\": tf.io.FixedLenFeature(\n","        [], dtype=tf.string, default_value=_EMPTY_STRING\n","    ),\n","    \"image/encoded\": tf.io.FixedLenFeature(\n","        [], dtype=tf.string, default_value=_EMPTY_IMAGE\n","    ),\n","    \"image/encoded_original\": tf.io.FixedLenFeature(\n","        [], dtype=tf.string, default_value=_EMPTY_IMAGE\n","    ),\n","    \"annotation_text\": tf.io.FixedLenFeature(\n","        [], dtype=tf.string, default_value=_EMPTY_STRING\n","    ),\n","    \"writing_guide\": tf.io.FixedLenFeature(\n","        [4], dtype=tf.float32, default_value=_EMPTY_BOUNDING_BOX\n","    ),\n","}\n","\n","def parse_single_example(elem):\n","  return tf.io.parse_single_example(elem, FEATURE_SPEC)\n","\n","def read_examples(file_path):\n","  \"\"\"Reads dataset examples from an sstable file.\"\"\"\n","  examples = {}\n","\n","  for sample in tf.data.TFRecordDataset([file_path]).map(parse_single_example):\n","    classname = sample['label'].numpy().decode()\n","    bbox = sample['annotation_bbox'].numpy()\n","    bbox = [\n","        int(bbox[0] * paligemma_tools.LOCATION_TOKENS_RANGE_MAX / _IMAGE_RESOLUTION),\n","        int(bbox[1] * paligemma_tools.LOCATION_TOKENS_RANGE_MAX / _IMAGE_RESOLUTION),\n","        int(bbox[2] * paligemma_tools.LOCATION_TOKENS_RANGE_MAX / _IMAGE_RESOLUTION),\n","        int(bbox[3] * paligemma_tools.LOCATION_TOKENS_RANGE_MAX / _IMAGE_RESOLUTION),\n","    ]\n","\n","    image = sample['image/encoded'].numpy()\n","    original_image = sample['image/encoded_original'].numpy()\n","    text = sample['annotation_text'].numpy().decode()\n","    label = (\n","        f'{classname} '\n","        f'{bbox[1]:.0f} {bbox[0]:.0f} '\n","        f'{bbox[3]:.0f} {bbox[2]:.0f} {text}'.strip()\n","    )\n","    ink_hash = sample['ink_hash'].numpy().decode()\n","    examples[ink_hash] = {\n","        'ink_hash': ink_hash,\n","        'bbox': convert_bbox(*bbox),\n","        'classname': classname,\n","        'composition_bbox': convert_bbox(*sample['composition_bbox'].numpy()),\n","        'image': Image.open(io.BytesIO(image)),\n","        'label': label,\n","        'original_image': Image.open(io.BytesIO(original_image)),\n","        'page_id': sample['example_id'].numpy().decode(),\n","        'text': text,\n","        'writing_guide': convert_bbox(*sample['writing_guide'].numpy()),\n","    }\n","\n","  return examples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3basI3NZ4syb"},"outputs":[],"source":["with open(INK_VALID_PATH, 'r') as f:\n","  ink_by_hash = {}\n","\n","  for ink_hash, ink_data in json.load(f).items():\n","    ink_by_hash[ink_hash] = document_editing.Ink(\n","        strokes=[\n","            document_editing.Stroke(xs=stroke['xs'], ys=stroke['ys'])\n","            for stroke in ink_data['strokes']\n","        ]\n","    )"]},{"cell_type":"markdown","metadata":{"id":"O8b9DN-q9EQt"},"source":["# Load all inks and documents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yj1Yvga_ZCH6"},"outputs":[],"source":["train_examples = read_examples(TF_TRAIN_PATH)\n","train_page_ids = [example['page_id'] for example in train_examples.values()]\n","valid_examples = read_examples(TF_VALID_PATH)\n","valid_page_ids = [example['page_id'] for example in valid_examples.values()]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwWEhk56ZGhn"},"outputs":[],"source":["valid_pages_data = [document_editing.load_page(PAGES_DIR, page_id) for page_id in tqdm(valid_page_ids)]\n","valid_pages = dict(zip(valid_page_ids, valid_pages_data))"]},{"cell_type":"markdown","metadata":{"id":"k13yOXeu7j0Q"},"source":["# Load the pre-trained PaLIGemma 2 model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVa6ZbIdtvkV"},"outputs":[],"source":["# Don't let TF use the GPU or TPUs\n","#tf.config.set_visible_devices([], \"GPU\")\n","#tf.config.set_visible_devices([], \"TPU\")\n","\n","backend = jax.extend.backend.get_backend()\n","print(f\"JAX version:  {jax.__version__}\")\n","print(f\"JAX platform: {backend.platform}\")\n","print(f\"JAX devices:  {jax.device_count()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"KN_WR4FVs8eB"},"outputs":[],"source":["# @title Initialize the PaLIGemma model.\n","\n","# Use these for PaliGemma-2 3B 448px²\n","LLM_VARIANT = \"gemma2_2b\"\n","\n","model_config = ml_collections.FrozenConfigDict({\n","    \"llm\": {\"vocab_size\": 257_152, \"variant\": LLM_VARIANT, \"final_logits_softcap\": 0.0},\n","    \"img\": {\"variant\": \"So400m/14\", \"pool_type\": \"none\", \"scan\": True, \"dtype_mm\": \"float16\"}\n","})\n","\n","model = paligemma.Model(**model_config)\n","tokenizer = sentencepiece.SentencePieceProcessor(PALIGEMMA_TOKENIZER_PATH)\n","paligemma_tokenizer = paligemma_tools.PaliGemmaTokenizer(tokenizer)\n","\n","# Load params - this can take up to 1 minute in T4 colabs.\n","params = paligemma.load(None, PALIGEMMA_MODEL_PATH, model_config)\n","\n","# Define `decode` function to sample outputs from the model.\n","decode_fn = predict_fns.get_all(model)['decode']\n","decode = functools.partial(decode_fn, devices=jax.devices(), eos_token=tokenizer.eos_id())"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"crRUw-20wy4F"},"outputs":[],"source":["# @title Move params to GPU/TPU memory.\n","#\n","# To keep HBM usage low and fit in a T4 GPU (16GB HBM) we opt to only finetune\n","# a part of the parameters. Additionally we keep the frozen params in float16\n","# and cast trainable to float32.\n","\n","# Create a pytree mask of the trainable params.\n","def is_trainable_param(name, param):  # pylint: disable=unused-argument\n","  if name.startswith(\"llm/layers/attn\"):  return True\n","  if name.startswith(\"llm/\"):              return False\n","  if name.startswith(\"img/\"):              return False\n","  raise ValueError(f\"Unexpected param name {name}\")\n","trainable_mask = big_vision.utils.tree_map_with_names(is_trainable_param, params)\n","\n","\n","# If more than one device is available (e.g. multiple GPUs) the parameters can\n","# be sharded across them to reduce HBM usage per device.\n","mesh = jax.sharding.Mesh(jax.devices(), (\"data\"))\n","\n","data_sharding = jax.sharding.NamedSharding(\n","    mesh, jax.sharding.PartitionSpec(\"data\"))\n","\n","params_sharding = big_vision.sharding.infer_sharding(\n","    params, strategy=[('.*', 'fsdp(axis=\"data\")')], mesh=mesh)\n","\n","# Yes: Some donated buffers are not usable.\n","warnings.filterwarnings(\n","    \"ignore\", message=\"Some donated buffers were not usable\")\n","\n","@functools.partial(jax.jit, donate_argnums=(0,), static_argnums=(1,))\n","def maybe_cast_to_f32(params, trainable):\n","  # Cast others to float16, since some GPUs don't support bf16.\n","  return jax.tree.map(lambda p, m: p.astype(jnp.float32)\n","                      if m else p.astype(jnp.float16),\n","                      params, trainable)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"c1Xw8yk_XVQg"},"outputs":[],"source":["# @title Trainable parameters mask.\n","trainable_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9Z5a6GSwmq9"},"outputs":[],"source":["# @title Move params to GPU/TPU memory.\n","\n","# Loading all params in simultaneous - albeit much faster and more succinct -\n","# requires more RAM than the T4 colab runtimes have by default (12GB RAM).\n","# Instead we do it param by param.\n","params, treedef = jax.tree.flatten(params)\n","sharding_leaves = jax.tree.leaves(params_sharding)\n","trainable_leaves = jax.tree.leaves(trainable_mask)\n","for idx, (sharding, trainable) in enumerate(zip(sharding_leaves, trainable_leaves)):\n","  #params[idx] = big_vision.utils.reshard(params[idx], sharding)\n","  params[idx] = maybe_cast_to_f32(params[idx], False) # trainable\n","  params[idx].block_until_ready()\n","params = jax.tree.unflatten(treedef, params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C1-P6DicQ02O"},"outputs":[],"source":["# @title Print parameters of the model.\n","def parameter_overview(params):\n","  for path, arr in big_vision.utils.tree_flatten_with_names(params)[0]:\n","    print(f\"{path:80s} {str(arr.shape):22s} {arr.dtype}\")\n","\n","print(\" == Model params == \")\n","parameter_overview(params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BK1Jf8LWpN3W"},"outputs":[],"source":["# @title Define the Evaluation/Inference loop.\n","def make_predictions(data_iterator,\n","                     postprocess_tokens,\n","                     num_examples=None,\n","                     batch_size=4, seqlen=paligemma_gesture_preparation._SEQLEN,\n","                     sampler=\"greedy\"):\n","  num_predicted = 0\n","  while True:\n","    # Construct a list of examples in the batch.\n","    examples = []\n","    try:\n","      for _ in range(batch_size):\n","        examples.append(next(data_iterator))\n","        examples[-1][\"_mask\"] = np.array(True)  # Indicates true example.\n","    except StopIteration:\n","      if len(examples) == 0:\n","        break\n","\n","    # Not enough examples to complete a batch. Pad by repeating last example.\n","    while len(examples) % batch_size:\n","      examples.append(dict(examples[-1]))\n","      examples[-1][\"_mask\"] = np.array(False)  # Indicates padding example.\n","\n","    # Convert list of examples into a dict of np.arrays and load onto devices.\n","    batch = jax.tree.map(lambda *x: np.stack(x), *examples)\n","    batch = big_vision.utils.reshard(batch, data_sharding)\n","\n","    # Make model predictions\n","    tokens = decode({\"params\": params}, batch=batch,\n","                    max_decode_len=seqlen, sampler=sampler)\n","\n","    # Fetch model predictions to device and detokenize.\n","    tokens, mask = jax.device_get((tokens, batch[\"_mask\"]))\n","    tokens = tokens[mask]  # remove padding examples.\n","    responses = [postprocess_tokens(t) for t in tokens]\n","\n","    if num_examples and num_predicted + len(responses) >= num_examples:\n","      responses = responses[:num_examples - num_predicted]\n","      yield from responses\n","      break\n","    yield from responses\n","    num_predicted += len(responses)"]},{"cell_type":"markdown","metadata":{"id":"fIIeqROKZnHG"},"source":["# Show predictions on validation dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WOqRPLCk3hLl"},"outputs":[],"source":["# @title Visualization utils\n","def format_label(prefix: str, label: str, color: str) -> str:\n","  parts = label.split(' ')\n","  class_name = parts[0]\n","  bbox = parts[1:5]\n","  text = parts[5:]\n","  return (\n","      f'<tt>{prefix} '\n","      f'<b>{class_name}</b> <span'\n","      f' style=\"background-color:{color}\">{\" \".join(bbox)}</span>'\n","      f' <span>{\" \".join(text)}</span></tt>'\n","  )\n","\n","def format_prediction(prefix: str, prediction: data_processing.DocumentEditingLabel, color: str) -> str:\n","  return (\n","      f'<tt>{prefix} '\n","      f'<b>{prediction.gesture}</b> <span'\n","      f' style=\"background-color:{color}\">{prediction.bbox.top:.0f} '\n","      f'{prediction.bbox.left:.0f} {prediction.bbox.bottom:.0f} '\n","      f'{prediction.bbox.right:.0f}</span>'\n","      f' <span>{prediction.text}</span></tt>'\n","  )\n","\n","def table_row(contents: list[list[str]]) -> str:\n","  cell_contents = ['<br/>'.join(content) for content in contents]\n","  cells = [\n","      '<td style=\"border: 1px solid lightgray; text-align:'\n","      f' center\">{cell_content}</td>'\n","      for cell_content in cell_contents\n","  ]\n","  return f'<tr>{\"\".join(cells)}</tr>'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjmIX294lpHF"},"outputs":[],"source":["examples_to_display = [\n","    (k, v)\n","    for k, v in valid_examples.items()\n","    if not v['label'].startswith('instruct')\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5pYNVwPJfHjE"},"outputs":[],"source":["def valid_data_iterator(examples_to_display):\n","  for _, example in examples_to_display:\n","    yield paligemma_gesture_preparation.prepare_inference_input(\n","        paligemma_tokenizer, example['image']\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OLVr11bGrZHU"},"outputs":[],"source":["predictions = []\n","valid_data_iter = valid_data_iterator(examples_to_display)\n","for pred in tqdm(make_predictions(valid_data_iter,\n","                     postprocess_tokens=paligemma_tokenizer.postprocess_tokens,\n","                     num_examples=16)):\n","  predictions.append(data_processing.DocumentEditingLabel.from_string(pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zT2UR-yVKl63"},"outputs":[],"source":["correct_class = []\n","for (page_id, example), prediction in zip(examples_to_display, predictions):\n","  if prediction is None:\n","    continue\n","  correct_class.append(prediction.gesture == example['classname'])\n","print(f'class accuracy {np.mean(correct_class)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mCOMHAuDAO_"},"outputs":[],"source":["for i, pred in enumerate(predictions):\n","  print(pred)\n","  print(examples_to_display[i][1]['label'])\n","  print('-----------')"]},{"cell_type":"markdown","metadata":{"id":"we2Ber4GtEsF"},"source":["**Task**\n","- Calculate IoU of predicted bounding boxes\n","- Calculate text metrics on recognition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VVFfqQr2aHWr"},"outputs":[],"source":["html = [\"\"\"\n","<table style=\"width: 1500px; border-collapse: collapse; border: 1px solid lightgray;\">\n","<thead>\n","  <tr>\n","    <th>Page</th>\n","    <th>Model output</th>\n","    <th>Document before</th>\n","    <th>Document after</th>\n","  </tr>\n","</thead>\n","<tbody>\n","\"\"\"]\n","\n","for (_, example), prediction_full in tqdm(zip(examples_to_display, predictions)):\n","  if prediction_full is None:\n","    continue\n","\n","  html_row_contents = []\n","  page = valid_pages[example['page_id']]\n","\n","  # Display the page ID with a rendering of the gesture.\n","  html_row_contents.append([\n","      f'<h4>{page_id}</h4>',\n","      rendering.to_html_image(example['original_image'], width=300),\n","  ])\n","\n","  # Ground truth and prediction strings.\n","  html_row_contents.append([\n","      format_label('Label:', example['label'], 'rgba(0, 255, 0, 0.3)'),\n","      format_prediction('Pred :', prediction_full, 'rgba(255, 0, 255, 0.3)'),\n","  ])\n","\n","  # Document before and after the edit.\n","  page_copy = copy.deepcopy(page)\n","\n","  bbox = example['bbox']\n","  composition_bbox = example['composition_bbox']\n","  writing_guide_bbox = example['writing_guide']\n","\n","  bbox = rendering.bbox_to_image_space(bbox, composition_bbox)\n","\n","  prediction = prediction_full.bbox\n","  prediction = rendering.bbox_to_image_space(prediction, composition_bbox)\n","  prediction_classname = prediction_full.gesture\n","  prediction_text = prediction_full.text\n","\n","  page_copy.edit(\n","      edit_name=prediction_classname,\n","      edit_bbox=prediction,\n","      text=prediction_text,\n","  )\n","\n","  # We compute first the \"after\" state, which returns an area of interest we can\n","  # crop around for better readability in the output table.\n","  rendering_after = rendering.render_document(\n","      page_copy,\n","      overlay_bboxes={'': composition_bbox},\n","      crop_area=True,\n","  )\n","  html_image_after = rendering.to_html_image(rendering_after.image, width=400)\n","\n","  rendering_before = rendering.render_document(\n","      page,\n","      overlay_bboxes={'lime': bbox, 'fuchsia': prediction},\n","      ink=ink_by_hash[example['ink_hash']],\n","      crop_area=rendering_after.area_of_interest,\n","  )\n","  html_image_before = rendering.to_html_image(rendering_before.image, width=400)\n","\n","  html_row_contents.append([html_image_before])\n","  html_row_contents.append([html_image_after])\n","  html.append(table_row(html_row_contents))\n","\n","html.append('</tbody></table>')\n","display(HTML(''.join(html)))"]},{"cell_type":"markdown","metadata":{"id":"f-9JG2yJnopD"},"source":["# Interactive Ink Canvas\n","In this section you can load one document from the dataset and play with the model. Draw a gesture with the mouse and click the `interpret` button to make a model call."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Kvn-6FRl9gq"},"outputs":[],"source":["# @title Prediction function for the interactive canvas\n","\n","\n","def canvas_predict_fn(ink: document_editing.Ink, image: Image.Image):\n","  # Prepare a square area around the gesture.\n","  ink_bbox = ink.get_bbox()\n","  size = max(ink_bbox.width, ink_bbox.height) + 40\n","  gesture_area = document_editing.BoundingBox(\n","      top=ink_bbox.center.y - size // 2,\n","      left=ink_bbox.center.x - size // 2,\n","      bottom=ink_bbox.center.y + size // 2,\n","      right=ink_bbox.center.x + size // 2,\n","  )\n","  model_input = image.crop((\n","      gesture_area.left,\n","      gesture_area.top,\n","      gesture_area.right,\n","      gesture_area.bottom,\n","  )).resize((_IMAGE_RESOLUTION, _IMAGE_RESOLUTION))\n","\n","  # Prepare the input for the model.\n","  composition = Image.new(\"RGBA\", model_input.size, \"white\")\n","  composition.paste(model_input, mask=model_input)\n","  inference_input = paligemma_gesture_preparation.prepare_inference_input(\n","      paligemma_tokenizer, image=composition\n","  )\n","\n","  prediction = next(\n","      make_predictions(\n","          iter([inference_input]),\n","          postprocess_tokens=paligemma_tokenizer.postprocess_tokens,\n","          batch_size=1,\n","          num_examples=1,\n","      )\n","  )\n","  parsed_prediction = data_processing.DocumentEditingLabel.from_output(\n","      prediction, loc_tokens=True\n","  )\n","  if not parsed_prediction:\n","    display(Javascript(f\"window.setModelOutput('(could not parse) {prediction}', '');\"))\n","    return data_processing.DocumentEditingLabel(\n","        gesture='none',\n","        bbox=document_editing.BoundingBox(top=0,left=0, bottom=0, right=0),\n","        text=''\n","    )\n","\n","  scale = _IMAGE_RESOLUTION / paligemma_tools.LOCATION_TOKENS_RANGE_MAX\n","\n","  # Show the predicted bounding box as an overlay to the input composition.\n","  draw = ImageDraw.Draw(composition)\n","  draw.rectangle(\n","      [\n","          (\n","              parsed_prediction.bbox.left * scale,\n","              parsed_prediction.bbox.top * scale,\n","          ),\n","          (\n","              parsed_prediction.bbox.right * scale,\n","              parsed_prediction.bbox.bottom * scale,\n","          ),\n","      ],\n","      outline=\"fuchsia\",\n","      width=2,\n","  )\n","\n","  # Show a view of the composition with the predicted bounding box on the side panel.\n","  composition_image_url = rendering.to_data_url(composition)\n","  display(Javascript(\n","          f\"window.setModelOutput('{parsed_prediction.to_string(data_processing.BBOX_FORMAT)}',\"\n","          f\" '{composition_image_url}');\"))\n","\n","  # Convert the predicted bounding box back to image space.\n","  parsed_prediction.bbox = rendering.bbox_to_image_space(\n","      parsed_prediction.bbox, gesture_area\n","  )\n","\n","  return parsed_prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECuE7CyGxCmY"},"outputs":[],"source":["one_page = document_editing.load_page(PAGES_DIR, '9790964376811024979')\n","one_page.tighten_bboxes_for_colab_canvas()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQkPVkOx2JA1"},"outputs":[],"source":["canvas = notebook_canvas.Canvas(one_page, canvas_predict_fn, canvas_max_width=800, canvas_max_height=1600)\n","canvas.display_interaction_widget()"]},{"cell_type":"markdown","metadata":{"id":"ju3FvYNlgWas"},"source":["# Synthetic dataset generation"]},{"cell_type":"markdown","metadata":{"id":"mIN_FmwN0aRj"},"source":["The trained model classifies and localizes a fixed set of classes. **What if we want to add a new class?**\n","\n","In this section we will show how to generate additional data with new class – **arrow**. As a first step we need a source of arrow inks. We use the [MathWriting](https://github.com/google-research/google-research/tree/master/mathwriting) dataset, that contains handwritten math formulas and extract LaTeX symbols\n","\n","$$\\leftrightarrow, \\Leftrightarrow$$\n","\n","Let's load the dataset (may take around 5 minutes)."]},{"cell_type":"markdown","metadata":{"id":"BvYIRxVCA7jE"},"source":["###  Download pre-made synthetic dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z12RRgDmBRjU"},"outputs":[],"source":["ARROWS_DIR = 'data/gestures/arrows'\n","download_workshop_data(os.path.join(ARROWS_DIR, 'arrows.zip'))"]},{"cell_type":"markdown","metadata":{"id":"D9depvslzdw4"},"source":["### Create dataset on the spot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQuwtoOn3V0G"},"outputs":[],"source":["# @title Load MathWriting dataset.\n","\n","MATHWRITING_FILE_NAME = 'mathwriting-2024.tgz'\n","MATHWRITING_BASE_PATH = \"data/mathwriting-2024\"\n","MATHWRITING_FILE_PATH = os.path.join(MATHWRITING_BASE_PATH, MATHWRITING_FILE_NAME)\n","\n","if not os.path.exists(MATHWRITING_BASE_PATH):\n","  !mkdir -p {MATHWRITING_BASE_PATH}\n","\n","!wget -nc https://storage.googleapis.com/mathwriting_data/mathwriting-2024.tgz --no-check-certificate -O {MATHWRITING_FILE_PATH}\n","shutil.unpack_archive(MATHWRITING_FILE_PATH, 'data/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YfyB4uYD54fW"},"outputs":[],"source":["# @title Extract arrows.\n","\n","def get_symbol_ink(symbol: arrow_ink_tools.InkPart) -> document_editing.Ink:\n","  \"\"\"Computes the actual ink from an InkPart object.\"\"\"\n","  ink = arrow_ink_tools.read_inkml_file(\n","      os.path.join(MATHWRITING_BASE_PATH, \"train\", f\"{symbol.source_sample_id}.inkml\"))\n","  strokes = [ink.strokes[i] for i in symbol.stroke_indices]\n","  return document_editing.Ink(strokes=strokes)\n","\n","import matplotlib.pyplot as plt\n","def plot_ink(ax: plt.Axes, ink: document_editing.Ink):\n","  \"\"\"Plots the ink data on the given axes.\"\"\"\n","  for stroke in ink.strokes:\n","    ax.plot(stroke.xs, stroke.ys, color=\"red\", linewidth=2, zorder=100)\n","\n","symbols = arrow_ink_tools.read_symbols_file(os.path.join(MATHWRITING_BASE_PATH, 'symbols.jsonl'))\n","arrows = [s for s in symbols if s.label in {'\\\\leftrightarrow', '\\\\Leftrightarrow'}]\n","print(f\"We've extracted {len(arrows)} unique arrows from MathWriting dataset\")"]},{"cell_type":"markdown","metadata":{"id":"N4VP2c4shb5D"},"source":["For each arrow we find **left and right critical points** – edges of an arrow."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TCApdvridgdP"},"outputs":[],"source":["arrows_downloaded = []\n","for i, arrow in enumerate(arrows):\n","  arrow_ink = arrow_ink_tools.ArrowInk(ink=get_symbol_ink(arrow))\n","  arrows_downloaded.append(arrow_ink)\n","  if i == 0:\n","    plt.close()\n","    plot_ink(ink=arrow_ink.ink, ax=plt)\n","    plt.plot(arrow_ink.left_critical_point.x, arrow_ink.left_critical_point.y, 'bo')\n","    plt.plot(arrow_ink.right_critical_point.x, arrow_ink.right_critical_point.y, 'bo')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnoC1s91LEFa"},"outputs":[],"source":["# @title Helper functions for sampling words and arrows.\n","def find_random_word_pair(page: document_editing.Page) -> tuple[int, int]:\n","  \"\"\"Finds two random words that are close on the page.\"\"\"\n","  word_ids = []\n","  for (id_, element) in page.element_from_id.items():\n","    if element.class_name == 'word':\n","      word_ids.append(id_)\n","  page_reader = rendering.render_document(page)\n","\n","  possible_pairs = []\n","  for i in range(len(word_ids)):\n","    for j in range(i + 1, len(word_ids)):\n","      c1 = page_example.element_from_id[word_ids[i]].bbox.center\n","      x1, y1 = c1.x, c1.y\n","      c2 = page_example.element_from_id[word_ids[j]].bbox.center\n","      x2, y2 = c2.x, c2.y\n","\n","      y_size = page_reader.extent.bottom - page_reader.extent.top\n","      x_size = page_reader.extent.right - page_reader.extent.left\n","      share_x = abs((x2 - x1) / x_size)\n","      share_y = abs((y2 - y1) / y_size)\n","\n","      if share_x <= 0.1 and share_y <= 0.1:\n","        possible_pairs.append((word_ids[i], word_ids[j]))\n","\n","  random_pair = random.choice(possible_pairs)\n","  y1 = page_example.element_from_id[random_pair[0]].bbox.center.y\n","  y2 = page_example.element_from_id[random_pair[1]].bbox.center.y\n","\n","  if y1 >= y2:\n","    return (random_pair[1], random_pair[0])\n","\n","  return random_pair\n","\n","def sample_two_words_and_fit_an_arrow(page_example):\n","  page_with_tight_bboxes = copy.deepcopy(page_example)\n","  page_with_tight_bboxes.tighten_bboxes_for_colab_canvas()\n","\n","  word_id1, word_id2 = find_random_word_pair(page_with_tight_bboxes)\n","  arrow = random.sample(arrows_downloaded, 1)[0]\n","  arrow_fitter = arrow_ink_tools.ArrowPageFitter(page_with_tight_bboxes, word_id1, word_id2)\n","  return arrow_fitter, arrow_fitter.fit_to_page(arrow=arrow, verbose=False)"]},{"cell_type":"markdown","metadata":{"id":"Eyb_Q8hCNgpl"},"source":["Our next step is to choose two random words on the page that are relatively close together. We then fit an arrow between them by aligning one of its endpoints with the center of the first word, and then rotating and scaling the arrow until its other endpoint aligns with the center of the second word."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WNM18HL-vJFL"},"outputs":[],"source":["dataset = []\n","num_tries = 3\n","for valid_page_id in tqdm(valid_page_ids):\n","  page_example = valid_pages[valid_page_id]\n","  for _ in range(num_tries):\n","    arrow_fitter, final_arrow = sample_two_words_and_fit_an_arrow(page_example)\n","    if arrow_ink_tools.check_that_arrow_is_located_correctly(arrow_fitter, final_arrow):\n","      dataset.append((arrow_fitter, final_arrow))\n","      break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z3Of20p5mvEe"},"outputs":[],"source":["# @title One example with an arrow on a page\n","arrow_fitter, final_arrow = dataset[6]\n","page_reader = rendering.render_document(\n","      arrow_fitter.page,\n","      overlay_bboxes={'lime': arrow_fitter.word_id1_bbox, 'fuchsia': arrow_fitter.word_id2_bbox,},\n","      ink=final_arrow.ink)\n","page_reader.image.show()"]},{"cell_type":"markdown","metadata":{"id":"6TbMi1yKNzgI"},"source":["Next, we crop images around each arrow (with a margin) and save them together with the targets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"McYWYlnpOwwa"},"outputs":[],"source":["# @title Save images on disk and prepare targets.\n","GENERATED_DATASET_PATH = os.path.join(MATHWRITING_BASE_PATH, 'arrow_dataset')\n","if not os.path.exists(GENERATED_DATASET_PATH):\n","  ! mkdir {GENERATED_DATASET_PATH}\n","\n","margin = 300\n","prepared_targets = []\n","for i, (arrow_fitter, final_arrow) in tqdm(enumerate(dataset)):\n","  center = final_arrow.ink.get_bbox().center\n","  x, y = center.x, center.y\n","  size = max(final_arrow.ink.get_bbox().width, final_arrow.ink.get_bbox().height) + margin\n","  crop_area = document_editing.BoundingBox(y - size // 2, x - size // 2, y + size // 2, x + size // 2)\n","  cropped_image = arrow_fitter.page.image_from_id[-1].crop(\n","      (crop_area.left, crop_area.top, crop_area.right, crop_area.bottom)\n","  )\n","  cropped_image.save(os.path.join(GENERATED_DATASET_PATH, f'{i+1}.png'))\n","  prepared_targets.append(arrow_ink_tools.get_arrow_target(arrow_fitter, crop_area))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPv3uyTWhjmh"},"outputs":[],"source":["print(f'Example of prepared target: {prepared_targets[0]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2TyFaM3Bfwc5"},"outputs":[],"source":["# @title Save jsonl files for datasets.\n","data_path = os.path.join(GENERATED_DATASET_PATH, 'data_train90.jsonl')\n","with open(data_path, 'w') as f:\n","  for i, target in enumerate(prepared_targets[:90]):\n","    json_data = {\"prefix\": \"\", \"suffix\": target, \"image\": f\"{i+1}.png\"}\n","    json.dump(json_data, f)\n","    f.write('\\n')\n","\n","data_path = os.path.join(GENERATED_DATASET_PATH, 'data_val10.jsonl')\n","with open(data_path, 'w') as f:\n","  for i, target in enumerate(prepared_targets[90:]):\n","    json_data = {\"prefix\": \"\", \"suffix\": target, \"image\": f\"{i+1}.png\"}\n","    json.dump(json_data, f)\n","    f.write('\\n')"]},{"cell_type":"markdown","metadata":{"id":"UFGPA6Q42uZf"},"source":["# PaLIGemma fine-tuning on synthetic data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NgWstW_6f7WD"},"outputs":[],"source":["# @title Load train and validation datasets from jsonl format.\n","\n","train_dataset = big_vision.datasets.jsonl.DataSource(\n","    os.path.join(GENERATED_DATASET_PATH, \"data_train90.jsonl\"),\n","    fopen_keys={\"image\": GENERATED_DATASET_PATH})\n","\n","val_dataset = big_vision.datasets.jsonl.DataSource(\n","    os.path.join(GENERATED_DATASET_PATH, \"data_val10.jsonl\"),\n","    fopen_keys={\"image\": GENERATED_DATASET_PATH})\n","\n","def train_data_iterator():\n","  \"\"\"Never ending iterator over training examples.\"\"\"\n","  # Shuffle examples and repeat so one can train for many epochs.\n","  dataset = train_dataset.get_tfdata().shuffle(1_000).repeat()\n","  for example in dataset.as_numpy_iterator():\n","    image = Image.open(io.BytesIO(example['image']))\n","    suffix = example['suffix'].decode()\n","    yield paligemma_gesture_preparation.prepare_train_input(paligemma_tokenizer, suffix=suffix, image=image)\n","\n","def val_data_iterator():\n","  \"\"\"Single iterator over validation examples..\"\"\"\n","  dataset = val_dataset.get_tfdata(ordered=True)\n","  for example in dataset.as_numpy_iterator():\n","    image = Image.open(io.BytesIO(example['image']))\n","    yield paligemma_gesture_preparation.prepare_inference_input(paligemma_tokenizer, image=image)"]},{"cell_type":"markdown","metadata":{"id":"eSchB_AuqEtT"},"source":["We check what the model currently outputs on arrows."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tvDeTTbguHl"},"outputs":[],"source":["for pred in make_predictions(val_data_iterator(),\n","                     postprocess_tokens=paligemma_tokenizer.postprocess_tokens,\n","                     num_examples=8):\n","  print(data_processing.DocumentEditingLabel.from_string(pred))"]},{"cell_type":"markdown","metadata":{"id":"3MOn6Dbkxm2Z"},"source":["We train the model the dataset with arrows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WU4eDQian8Nf"},"outputs":[],"source":["# @title Define the training step.\n","#\n","# The main update_fn using simple SGD.\n","#\n","@functools.partial(jax.jit, donate_argnums=(0,))\n","def update_fn(params, batch, learning_rate):\n","  imgs, txts, mask_ar = batch[\"image\"], batch[\"text\"], batch[\"mask_ar\"]\n","\n","  def loss_fn(params):\n","    text_logits, _ = model.apply({\"params\": params}, imgs, txts[:, :-1], mask_ar[:, :-1], train=True)\n","    logp = jax.nn.log_softmax(text_logits, axis=-1)\n","\n","    # The model takes as input txts[:, :-1] but the loss is defined as predicting\n","    # next tokens txts[:, 1:]. Additionally, mask_loss[:, 1:] indicates which tokens\n","    # are part of the loss (e.g. prefix and padded tokens are not included).\n","    mask_loss = batch[\"mask_loss\"][:, 1:]\n","    targets = jax.nn.one_hot(txts[:, 1:], text_logits.shape[-1])\n","\n","    # Compute the loss per example. i.e. the mean of per token pplx.\n","    # Since each example has a different number of tokens we normalize it.\n","    token_pplx = jnp.sum(logp * targets, axis=-1)  # sum across vocab_size.\n","    example_loss = -jnp.sum(token_pplx * mask_loss, axis=-1)  # sum across seq_len.\n","    example_loss /= jnp.clip(jnp.sum(mask_loss, -1), 1)  # weight by num of tokens.\n","\n","    # batch_loss: mean of per example loss.\n","    return jnp.mean(example_loss)\n","\n","  loss, grads = jax.value_and_grad(loss_fn)(params)\n","\n","  # Apply gradients to trainable params using SGD.\n","  def apply_grad(param, gradient, trainable):\n","    if not trainable: return param\n","    return param - learning_rate * gradient\n","\n","  params = jax.tree_util.tree_map(apply_grad, params, grads, trainable_mask)\n","\n","  return params, loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQtu0ZMMximQ"},"outputs":[],"source":["# @title Run training loop.\n","#\n","# Run a short training loop with cosine learning rate schedule.\n","#\n","# Note: the first step can be quite slow on some machines (up to several minutes)\n","# due to XLA compilation of the jax.jit'd function.\n","\n","BATCH_SIZE = 1\n","LEARNING_RATE = 0.0001\n","\n","TRAIN_STEPS = 128\n","EVAL_STEPS = 32\n","NUM_EVAL_EXAMPLES = 4\n","\n","# collect valid targets\n","dataset = val_dataset.get_tfdata(ordered=True)\n","eval_targets = []\n","for example in dataset.as_numpy_iterator():\n","  eval_targets.append(example['suffix'])\n","  if len(eval_targets) == NUM_EVAL_EXAMPLES:\n","    break\n","\n","train_data_it = train_data_iterator()\n","\n","sched_fn = big_vision.utils.create_learning_rate_schedule(\n","    total_steps=TRAIN_STEPS+1, base=LEARNING_RATE,\n","    decay_type=\"cosine\", warmup_percent=0.10)\n","\n","for step in range(1, TRAIN_STEPS+1):\n","  # Make list of N training examples.\n","  examples = [next(train_data_it) for _ in range(BATCH_SIZE)]\n","\n","  # Convert list of examples into a dict of np.arrays and load onto devices.\n","  batch = jax.tree.map(lambda *x: np.stack(x), *examples)\n","  #batch = big_vision.utils.reshard(batch, data_sharding)\n","\n","  # Training step and report training loss\n","  learning_rate = sched_fn(step)\n","  params, loss = update_fn(params, batch, learning_rate)\n","\n","  loss = jax.device_get(loss)\n","  print(f\"step: {step:2d}/{TRAIN_STEPS:2d}   lr: {learning_rate:.5f}   loss: {loss:.4f}\")\n","\n","  if step == 1 or (step % EVAL_STEPS) == 0:\n","    print(f\"Model predictions at step {step}\")\n","    for pred, target in zip(make_predictions(val_data_iterator(),\n","                                 postprocess_tokens=paligemma_tokenizer.postprocess_tokens,\n","                                 num_examples=NUM_EVAL_EXAMPLES), eval_targets):\n","      print('predicition:', pred)\n","      print('target:', target)"]},{"cell_type":"markdown","metadata":{"id":"EQZxmkiPuxOn"},"source":["# Few-shot with Gemini\n","In this part, we will focus on trying to use bigger Foundational Models (e.g. Gemini 2.0) to solve the document editing task through 0-shot/few-shot approaches."]},{"cell_type":"markdown","metadata":{"id":"Fd4bHZ-2vNuM"},"source":["## Data Loading\n","\n","We will load the training and validation dataset for document editing. We will use the training set to sample the different few-shot examples, and run the evaluation on the validation dataset. The few-shot examples are sampled in a stratified way (where stratas are defined as the gesture types), to ensure the model sees examples of each of the classes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1fYTsPTFvPCB"},"outputs":[],"source":["# Validation dataset\n","eval_samples = valid_examples.values()\n","valid_dataset = tf.data.Dataset.from_tensor_slices(\n","    {\n","      'ink_hash': np.array([example[\"ink_hash\"] for example in eval_samples]),\n","      'image/encoded': np.array([example[\"image\"] for example in eval_samples]),\n","      'label': np.array([example[\"label\"] for example in eval_samples]),\n","      'image_width': np.array([example[\"original_image\"].width for example in eval_samples]),\n","      'image_height': np.array([example[\"original_image\"].height for example in eval_samples]),\n","    }\n",")\n","\n","# Train dataset\n","train_samples = train_examples.values()\n","train_dataset = tf.data.Dataset.from_tensor_slices(\n","    {\n","      'ink_hash': np.array([example[\"ink_hash\"] for example in train_samples]),\n","      'image/encoded': np.array([example[\"image\"] for example in train_samples]),\n","      'label': np.array([example[\"label\"] for example in train_samples]),\n","      'image_width': np.array([example[\"original_image\"].width for example in train_samples]),\n","      'image_height': np.array([example[\"original_image\"].height for example in train_samples]),\n","    }\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjUCBm2-vRJM"},"outputs":[],"source":["def get_gesture(example):\n","  return tf.strings.split(example[\"label\"], \" \", 1)[0]\n","\n","def get_normalization_factor(example):\n","  return (example['image_width'] / 1000, example['image_height'] / 1000)\n","\n","\n","shot_sampler = sampling.StratifiedSampler(train_dataset, get_gesture)"]},{"cell_type":"markdown","metadata":{"id":"D_8sOrGmvSqy"},"source":["## Model Loading\n","\n","This cell loads the Gemini Model API Client and the corresponding inference function to be used later on to infer the prediction for a given document gesture."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdXYIX98vVY_"},"outputs":[],"source":["# You need to create one on https://aistudio.google.com/app/apikey .\n","API_KEY = \"..\"\n","MODEL_NAME = \"gemini-1.5-pro\"\n","\n","client = model_api.get_client(API_KEY)\n","inference_fn = model_api.client_to_inference_fn(client, MODEL_NAME)"]},{"cell_type":"markdown","metadata":{"id":"4qcVfxLwvwiu"},"source":["## Few Shot Prompt Definition\n","\n","In this section, we focus on defining the prompt format that will be used for querying Gemini in 0-shot/few-shot settings. We provide some helper classes, functions and overall template for defining the prompt, but feel free to play around with them in different manners to reach the best possible results!\n","\n","The provided template is composed of 3 main parts:\n","- The `prefix` variable which corresponds to the main instruction given to the model.\n","- The `shot_prefix` variable which will add text to the prefix if there is at least 1 shot example provided.\n","- The `GestureFewShotPrompter.prepare_example` function which defines the format into which examples are to be generated to be added to the prompt (when `is_shot` is `True`) and used for formatting the current inference example (when `is_shot` is `False`).\n","\n","\n","Note: Gemini normalizes the coordinate system to be `([0, 1000], [0, 1000])` instead of `([0, width], [0, height])` and prefers being prompted with y coordinate first and x coordinate second. Additionally, the original dataset contains special tokens for locations in the form \\<locXXXX\\> which range from 0 to 1023, which we have already transformed for you."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M1t_kAqrvZtd"},"outputs":[],"source":["class GestureFewShotPrompter(in_context_learning.FewShotPrompter):\n","  def prepare_example(self, example, is_shot):\n","    data = []\n","\n","    image = self.load_image(example[\"image/encoded\"])\n","    data.append(image)\n","\n","    if is_shot:\n","      label = example[\"label\"].decode()\n","      data.append(label)\n","\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2qPl70RZv0A9"},"outputs":[],"source":["GESTURES = (\n","    'crop',\n","    'delete',\n","    'insert',\n","    'instruct_image',\n","    'instruct_text',\n","    'question',\n","    'select',\n","    'underline',\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-0zjkVtov04-"},"outputs":[],"source":["prefix = (\n","f\"\"\"You receive as input an image that contains some text and a human gesture in red ink strokes.\n","\n","Your task is to predict the type of gesture, the bounding box of the text it is annotating with a transcription of what the user wrote in handwriting (if anything was written) in the following format <gesture> <ymin> <xmin> <ymax> <xmax> [<transcription>].\n","The bounding box should have normalized coordinates as int [0, 1000). (0, 0) is the top left corner and the y, x coordinate values are relative values with respect to image height and width. Only output the gesture type, bounding box and text (if present) and nothing else.\n","\n","The possible gestures are:\n","{os.linesep.join(f\"- {gesture}\" for gesture in GESTURES)}\n","\n","\"\"\"\n",")\n","shot_prefix = \"\"\"Now, we show you some examples for each gesture type.\n","\n","\"\"\"\n","\n","example = next(iter(valid_dataset.as_numpy_iterator()))\n","n_shots = 2 # @param\n","shots = list(shot_sampler.sample(num_examples=n_shots))\n","shots_gemini = [\n","    data_processing.transform_example_label(\n","        example,\n","        data_processing.BBOX_PATTERN,\n","        data_processing.BBOX_FORMAT,\n","        get_normalization_factor(example)\n","    )\n","    for example in shots\n","]\n","prompter = GestureFewShotPrompter(prefix, shot_prefix, shots_gemini)\n","prompter.display_prompt(example)"]},{"cell_type":"markdown","metadata":{"id":"0Jg4AfSBv3Kr"},"source":["## Fewshot Inference\n","\n","Based on the shot prompter you defined above, the following cells will run inference on a different number of shots. Feel free to modify your prompt to try and reach the best results!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q5haOmhRv4B5"},"outputs":[],"source":["results = {}\n","confusion_matrix = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qFXRJ_u1v5Iu"},"outputs":[],"source":["N_SHOTS = [0, 1, 2, 4, 8]\n","\n","for n_shots in N_SHOTS:\n","  shots = list(shot_sampler.sample(num_examples=n_shots))\n","  shots_gemini = [\n","      data_processing.transform_example_label(\n","          example,\n","          data_processing.BBOX_PATTERN,\n","          data_processing.BBOX_FORMAT,\n","          get_normalization_factor(example)\n","      )\n","      for example in shots\n","  ]\n","  prompter = GestureFewShotPrompter(prefix, shot_prefix, shots_gemini)\n","  targets, predictions = in_context_learning.infer_fewshot(\n","      inference_fn,\n","      prompter,\n","      valid_dataset,\n","      normalize_fn=get_normalization_factor\n","  )\n","\n","  accuracies, ious, cers = metrics.compute_document_editing_metrics(targets, predictions)\n","  cm = metrics.confusion_matrix(predictions, targets)\n","\n","  results[n_shots] = {\n","      'Accuracy': sum(accuracies) / len(accuracies),\n","      'IoU': sum(ious) / len(ious),\n","      'CER': sum(cers) / len(cers),\n","  }\n","  confusion_matrix[n_shots] = cm\n","\n","metrics.plot_fewshot_results(results)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["dgx4QQ_Mq7AT","F7YYSOL58CbE","JwMKHCEK8GPI","Kz6gaNiD6Wmg","O8b9DN-q9EQt","k13yOXeu7j0Q","fIIeqROKZnHG","f-9JG2yJnopD","EQZxmkiPuxOn"],"gpuType":"T4","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
