{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAGnoVRAzlo5"
      },
      "source": [
        "# Ink Interactions @ AMLD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDzroEPTOC1S"
      },
      "source": [
        "# Section 1 - Introduction\n",
        "In this colab you will be able to interact with a \"webpage\"-like document using ink. To this end, we will experiment with different models and approaches on the following task:\n",
        "\n",
        "Given an image of the webpage with a rendered gesture on top of it, identify:\n",
        "- The **type of the gesture**\n",
        "- The **target of the gesture** (i.e. its bounding box)\n",
        "- Additional **text instructions**\n",
        "\n",
        "By constraining this problem on a specific set of gestures with pre-defined\n",
        "behavior, we can interact/update/change the document based on the\n",
        "predicted attributes of a gesture.\n",
        "\n",
        "We will work with the following gestures:\n",
        "- **insert**: to insert text at a given region\n",
        "- **select**: to highlight parts of the webpage\n",
        "- **question**: to get clarifications about a part of the webpage\n",
        "- **delete**: to delete parts of the webpage\n",
        "- **crop**: to crop a region of the webpage\n",
        "- **underline**: to highlight parts of the webpage\n",
        "- **instruct**: to interact with the content (image/text) via instruction\n",
        "\n",
        "\u003cimg src=\"https://storage.googleapis.com/amld_workshop_natural_interactions_with_llms/gesture_classes.png\" height=\"400\"/\u003e\n",
        "\n",
        "------\n",
        "\n",
        "Note that a given type of gesture can have multiple variants. For example, a `delete` operation can be done by drawing a squiggly line, by striking out the text with a single horizontal stroke or by crossing out the text. The pictures above only show a single variant per class. If you are interested, you can look at the TFRecord dataset contained in `data/gestures` (see below, the part about downloading the datasets for this colab).\n",
        "\n",
        "As described above, the original training set for the model consists of images of webpages taken from Wikipedia with some handwritten gesture rendered on top. Since images are static, we converted them into a very (very) crude document structure. This way, we will be able to simulate how an \"interaction\" with the model looks like. A document is essentially a collection of words, text lines and paragraphs represented with a location and some size in JSON format:\n",
        "\n",
        "\n",
        "```\n",
        "\"elements\": [\n",
        "  {\n",
        "    \"id\": 1,\n",
        "    \"parent_id\": 0,\n",
        "    \"class_name\": \"textline\",\n",
        "    \"bbox\": {\"left\": 23, \"top\": 35, \"right\": 384, \"bottom\": 63},\n",
        "    \"children_ids\": [2, 3, 4, …]\n",
        "  },\n",
        "  {\n",
        "    \"id\": 2,\n",
        "    \"parent_id\": 1,\n",
        "    \"class_name\": \"word\",\n",
        "    \"text\": \"foo\",\n",
        "    \"bbox\": {\"left\": 23, \"top\": 35, \"right\": 185, \"bottom\": 63},\n",
        "  },\n",
        "  …\n",
        "]\n",
        "```\n",
        "\n",
        "We predefined for you a series of simple document \"edits\" (available in the `document_editing` module) that can be triggered using the output of the model. These are mostly intended for illustrative purposes and to play with the model in the [interactive ink canvas](#scrollTo=f-9JG2yJnopD). In the figure below for example, we used for example the `insert` tool to add some text to the paragraph. In these visualizations, each individual element is represented by its bounding box (light blue for words, blue for text lines an orange for paragraphs) and text content.\n",
        "\n",
        "You will notice that they look quite different from the data that the model has seen during training. However, in practice the model is quite robust to small differences in rendering.\n",
        "\n",
        "\u003cimg src=\"https://storage.googleapis.com/amld_workshop_natural_interactions_with_llms/document_editing_illustration.png\" height=\"400\"/\u003e\n",
        "\n",
        "As part of this workshop and in addition to using the model to perform known tasks, you will:\n",
        "- Experiment with in-context learning through zero-/few-shot approaches with Gemini to solve the tasks without any fine-tuning\n",
        "- Attempt to define a brand new task with a novel gesture class and fine-tune the pre-trained model on it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgx4QQ_Mq7AT"
      },
      "source": [
        "## Setting up code repositories and datasets.\n",
        "\n",
        "This section imports all the code and downloads all the data required for the workshop. You can explore the assets used by this notebook by clicking on the navigation bar on the left. All datasets are located under `/content/data` while model files are located under `/content/models`. This will take a few minutes ⏳!\n",
        "\n",
        "**Important note**: you may need to run `!gcloud init` in a cell the first time you use the `gcloud` command.\n",
        "\n",
        "⏩ You do not need to interact with the code in the **following cells**, you may therefore **keep them collapsed** to keep the colab as readable as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7YYSOL58CbE"
      },
      "source": [
        "### Code setup\n",
        "Import code and clone repositories required for the workshop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cwx-pBVsPvet"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import functools\n",
        "import glob\n",
        "import io\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "from IPython.display import HTML, Javascript, display\n",
        "from PIL import Image, ImageDraw\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# The T4 runtime is tight on memory to finetune this model. Preallocate\n",
        "# all memory ahead of time to avoid OOM'ing due to fragmentation.\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WNZGsLE0G9O"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/google-deepmind/amld_workshop_natural_interactions_with_llms.git\n",
        "!git clone --branch=main --depth=1 https://github.com/google-research/big_vision big_vision_repository\n",
        "\n",
        "# Install libraries needed for cairo.\n",
        "!apt-get install -q libcairo2-dev libjpeg-dev libgif-dev\n",
        "\n",
        "# Install missing dependencies. Assume jax~=0.4.25 with GPU available.\n",
        "!pip3 install -q \"crcmod\" \"overrides\" \"ml_collections\" \"einops~=0.7\" \"sentencepiece\" \"jiwer\" \"pycairo\"\n",
        "\n",
        "sys.path.append('amld_workshop_natural_interactions_with_llms')\n",
        "sys.path.append(\"big_vision_repository\")\n",
        "\n",
        "import ml_collections\n",
        "import sentencepiece\n",
        "\n",
        "import big_vision\n",
        "import jax\n",
        "import big_vision.models.vit\n",
        "from big_vision.models.proj.paligemma import gemma_bv\n",
        "from big_vision.models.proj.paligemma import paligemma\n",
        "from big_vision.trainers.proj.paligemma import predict_fns\n",
        "import big_vision.datasets.jsonl\n",
        "import big_vision.utils\n",
        "import big_vision.sharding\n",
        "import jax.numpy as jnp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cY215aA7jeP"
      },
      "source": [
        "Re-execute this cell if you make changes to the code under `/content/amld_workshop_natural_interactions_with_llms`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZpvPLVN7bt2"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import arrow_ink_tools\n",
        "import data_processing\n",
        "import document_editing\n",
        "import in_context_learning\n",
        "import metrics\n",
        "import model_api\n",
        "import notebook_canvas\n",
        "import paligemma_gesture_preparation\n",
        "import paligemma_tools\n",
        "import rendering\n",
        "import sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwMKHCEK8GPI"
      },
      "source": [
        "### Data setup\n",
        "This will download all the workshop data to the local filesystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KQ45tpiuwwx"
      },
      "outputs": [],
      "source": [
        "INK_GCP_BUCKET_URL = 'gs://amld_workshop_natural_interactions_with_llms'\n",
        "BIG_VISION_BUCKET_URL = 'gs://big_vision'\n",
        "\n",
        "PAGES_DIR = 'data/wikipedia_public'\n",
        "INK_VALID_PATH = 'data/gestures/ink_gestures_valid.json'\n",
        "INK_TRAIN_PATH = 'data/gestures/ink_gestures_train.json'\n",
        "TF_VALID_PATH = 'data/gestures/annotation_gestures_valid.tfrecord'\n",
        "TF_TRAIN_PATH = 'data/gestures/annotation_gestures_train.tfrecord'\n",
        "PALIGEMMA_MODEL_PATH = 'models/paligemma_ft_digink_448.b16.npz'\n",
        "PALIGEMMA_TOKENIZER_PATH = 'models/paligemma_tokenizer.model'\n",
        "\n",
        "\n",
        "def download_workshop_data(file_path: str, bucket=INK_GCP_BUCKET_URL):\n",
        "  file_name = os.path.basename(file_path)\n",
        "  if os.path.exists(file_path):\n",
        "    return\n",
        "\n",
        "  print(f'Downloading {file_name} into {file_path}.')\n",
        "  if file_name.endswith('.zip'):\n",
        "    !gsutil cp {os.path.join(bucket, file_name)} {file_path}\n",
        "    !unzip -q {file_path} -d {os.path.dirname(file_path)}\n",
        "  else:\n",
        "    !gsutil cp {os.path.join(bucket, file_name)} {file_path}\n",
        "  print()\n",
        "\n",
        "download_workshop_data(os.path.join(PAGES_DIR, 'wikipedia_public.zip'))\n",
        "download_workshop_data(INK_TRAIN_PATH)\n",
        "download_workshop_data(INK_VALID_PATH)\n",
        "download_workshop_data(TF_TRAIN_PATH)\n",
        "download_workshop_data(TF_VALID_PATH)\n",
        "download_workshop_data(PALIGEMMA_MODEL_PATH)\n",
        "download_workshop_data(PALIGEMMA_TOKENIZER_PATH, bucket=BIG_VISION_BUCKET_URL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz6gaNiD6Wmg"
      },
      "source": [
        "### Utility functions\n",
        "This section defines additional utility functions to load, manipulate and visualize the data that we just downloaded and that we will be using during the workshop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3QkNTjfXPpf"
      },
      "outputs": [],
      "source": [
        "_IMAGE_RESOLUTION = 448\n",
        "\n",
        "def convert_bbox(\n",
        "    left: float, top: float, right: float, bottom: float\n",
        ") -\u003e document_editing.BoundingBox:\n",
        "  \"\"\"Convert a bounding box from a list of integers to a dataclass instance.\"\"\"\n",
        "  return document_editing.BoundingBox(\n",
        "      top=top,\n",
        "      left=left,\n",
        "      bottom=bottom,\n",
        "      right=right,\n",
        "  )\n",
        "\n",
        "_EMPTY_STRING = tf.convert_to_tensor(\"\", dtype=\"string\").numpy()\n",
        "_EMPTY_BOUNDING_BOX = np.array([0.0, 0.0, 0.0, 0.0], dtype=\"float32\")\n",
        "_EMPTY_IMAGE = tf.io.encode_png(tf.constant([[[0]]], dtype=\"uint8\")).numpy()\n",
        "FEATURE_SPEC = {\n",
        "    \"ink_hash\": tf.io.FixedLenFeature(\n",
        "        [], dtype=tf.string, default_value=_EMPTY_STRING\n",
        "    ),\n",
        "    \"example_id\": tf.io.FixedLenFeature(\n",
        "        [], dtype=tf.string, default_value=_EMPTY_STRING\n",
        "    ),\n",
        "    \"annotation_bbox\": tf.io.FixedLenFeature(\n",
        "        [4], dtype=tf.float32, default_value=_EMPTY_BOUNDING_BOX\n",
        "    ),\n",
        "    \"composition_bbox\":  tf.io.FixedLenFeature(\n",
        "        [4], dtype=tf.float32, default_value=_EMPTY_BOUNDING_BOX\n",
        "    ),\n",
        "    \"label\": tf.io.FixedLenFeature(\n",
        "        [], dtype=tf.string, default_value=_EMPTY_STRING\n",
        "    ),\n",
        "    \"image/encoded\": tf.io.FixedLenFeature(\n",
        "        [], dtype=tf.string, default_value=_EMPTY_IMAGE\n",
        "    ),\n",
        "    \"image/encoded_original\": tf.io.FixedLenFeature(\n",
        "        [], dtype=tf.string, default_value=_EMPTY_IMAGE\n",
        "    ),\n",
        "    \"annotation_text\": tf.io.FixedLenFeature(\n",
        "        [], dtype=tf.string, default_value=_EMPTY_STRING\n",
        "    ),\n",
        "    \"writing_guide\": tf.io.FixedLenFeature(\n",
        "        [4], dtype=tf.float32, default_value=_EMPTY_BOUNDING_BOX\n",
        "    ),\n",
        "}\n",
        "\n",
        "def parse_single_example(elem):\n",
        "  return tf.io.parse_single_example(elem, FEATURE_SPEC)\n",
        "\n",
        "def read_examples(file_path):\n",
        "  \"\"\"Reads dataset examples from an sstable file.\"\"\"\n",
        "  examples = {}\n",
        "\n",
        "  for sample in tf.data.TFRecordDataset([file_path]).map(parse_single_example):\n",
        "    classname = sample['label'].numpy().decode()\n",
        "    bbox = sample['annotation_bbox'].numpy()\n",
        "    bbox = [\n",
        "        int(bbox[0] * paligemma_tools.LOCATION_TOKENS_RANGE_MAX / _IMAGE_RESOLUTION),\n",
        "        int(bbox[1] * paligemma_tools.LOCATION_TOKENS_RANGE_MAX / _IMAGE_RESOLUTION),\n",
        "        int(bbox[2] * paligemma_tools.LOCATION_TOKENS_RANGE_MAX / _IMAGE_RESOLUTION),\n",
        "        int(bbox[3] * paligemma_tools.LOCATION_TOKENS_RANGE_MAX / _IMAGE_RESOLUTION),\n",
        "    ]\n",
        "\n",
        "    image = sample['image/encoded'].numpy()\n",
        "    original_image = sample['image/encoded_original'].numpy()\n",
        "    text = sample['annotation_text'].numpy().decode()\n",
        "    label = (\n",
        "        f'{classname} '\n",
        "        f'{bbox[1]:.0f} {bbox[0]:.0f} '\n",
        "        f'{bbox[3]:.0f} {bbox[2]:.0f} {text}'.strip()\n",
        "    )\n",
        "    ink_hash = sample['ink_hash'].numpy().decode()\n",
        "    examples[ink_hash] = {\n",
        "        'ink_hash': ink_hash,\n",
        "        'bbox': convert_bbox(*bbox),\n",
        "        'classname': classname,\n",
        "        'composition_bbox': convert_bbox(*sample['composition_bbox'].numpy()),\n",
        "        'image': Image.open(io.BytesIO(image)),\n",
        "        'label': label,\n",
        "        'original_image': Image.open(io.BytesIO(original_image)),\n",
        "        'page_id': sample['example_id'].numpy().decode(),\n",
        "        'text': text,\n",
        "        'writing_guide': convert_bbox(*sample['writing_guide'].numpy()),\n",
        "    }\n",
        "\n",
        "  return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3basI3NZ4syb"
      },
      "outputs": [],
      "source": [
        "with open(INK_VALID_PATH, 'r') as f:\n",
        "  ink_by_hash = {}\n",
        "\n",
        "  for ink_hash, ink_data in json.load(f).items():\n",
        "    ink_by_hash[ink_hash] = document_editing.Ink(\n",
        "        strokes=[\n",
        "            document_editing.Stroke(xs=stroke['xs'], ys=stroke['ys'])\n",
        "            for stroke in ink_data['strokes']\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8b9DN-q9EQt"
      },
      "source": [
        "## Load all inks and documents\n",
        "This will read all the training and validation examples in [TFExample](https://www.tensorflow.org/api_docs/python/tf/train/Example?) format and prepare `document_editing.Page`'s that represents documents with their elements and images.\n",
        "\n",
        "⏩ You do not need to interact with the code in the **following cells**, you may therefore **keep them collapsed** to keep the colab as readable as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj1Yvga_ZCH6"
      },
      "outputs": [],
      "source": [
        "train_examples = read_examples(TF_TRAIN_PATH)\n",
        "train_page_ids = [example['page_id'] for example in train_examples.values()]\n",
        "valid_examples = read_examples(TF_VALID_PATH)\n",
        "valid_page_ids = [example['page_id'] for example in valid_examples.values()]\n",
        "\n",
        "valid_pages_data = [document_editing.load_page(PAGES_DIR, page_id) for page_id in tqdm(valid_page_ids)]\n",
        "valid_pages = dict(zip(valid_page_ids, valid_pages_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k13yOXeu7j0Q"
      },
      "source": [
        "## Load the pre-trained PaLIGemma 2 model for gesture recognition\n",
        "\n",
        "This part will initialize a [PaLIGemma](https://ai.google.dev/gemma/docs/paligemma) (VLM) model trained on gesture recognition. This checkpoint was obtained by taking a standard PaLIGemma pre-trained model and fine-tuning it on a ~50/50 mixture of two tasks:\n",
        "\n",
        "1. A gesture recognition task, where the model has to interpret a gesture, given the image of an annotated document.\n",
        "1. A text detection task, where the model has to output a bounding box, given some text that appears on the image.\n",
        "\n",
        "\n",
        "For gesture recognition, the target is a string formatted like:\n",
        "\n",
        "```\n",
        "\u003ctype of gesture\u003e \u003cthe gesture's target bounding box\u003e \u003cdetected text of the gesture\u003e\n",
        "```\n",
        "\n",
        "For example:\n",
        "\n",
        "\u003cimg src=\"https://storage.googleapis.com/amld_workshop_natural_interactions_with_llms/gesture_recognition_task.png\" height=\"300\"/\u003e\n",
        "\n",
        "For the text detection task, the target is just the bounding box but the input is an image and some text appearing in the image.\n",
        "\n",
        "\u003cimg src=\"https://storage.googleapis.com/amld_workshop_natural_interactions_with_llms/text_detection_task.png\" height=\"300\"/\u003e\n",
        "\n",
        "When training this model, we noticed that adding the text detection task to the mixture turned out to be one of the most important factors in getting it to output accurate bounding boxes around text elements. This also made the model more robust in detecting the target of the gesture, despite users drawing them imprecisely (for example, by including or omitting neighboring characters in circlings).\n",
        "\n",
        "Also, you might have noticed that the background in the gesture recognition task appears slightly transparent. This is just a simple trick that helps to separate the gesture overlay from the rest of the scene, making it easier for the model to \"see\". This transparency is safe for inference, as the document and gesture overlay are distinct rendering layers that we fully control.\n",
        "\n",
        "\n",
        "This code is based on the example colab from the creators of PaLIGemma https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/finetune_paligemma.ipynb\n",
        "\n",
        "**Important** You can reload the model by rerunning all cells in this section.\n",
        "\n",
        "⏩ This code will be discussed in the subsequent section on fine-tuning ↩️ . Loading the model from disk will take a few minutes ⏳!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KN_WR4FVs8eB"
      },
      "outputs": [],
      "source": [
        "# @title Initialize the PaLIGemma model.\n",
        "\n",
        "# Don't let TF use the GPU or TPUs\n",
        "#tf.config.set_visible_devices([], \"GPU\")\n",
        "#tf.config.set_visible_devices([], \"TPU\")\n",
        "\n",
        "backend = jax.extend.backend.get_backend()\n",
        "print(f\"JAX version:  {jax.__version__}\")\n",
        "print(f\"JAX platform: {backend.platform}\")\n",
        "print(f\"JAX devices:  {jax.device_count()}\")\n",
        "\n",
        "# Use these for PaliGemma-2 3B 448px²\n",
        "LLM_VARIANT = \"gemma2_2b\"\n",
        "\n",
        "model_config = ml_collections.FrozenConfigDict({\n",
        "    \"llm\": {\"vocab_size\": 257_152, \"variant\": LLM_VARIANT, \"final_logits_softcap\": 0.0},\n",
        "    \"img\": {\"variant\": \"So400m/14\", \"pool_type\": \"none\", \"scan\": True, \"dtype_mm\": \"float16\"}\n",
        "})\n",
        "\n",
        "model = paligemma.Model(**model_config)\n",
        "tokenizer = sentencepiece.SentencePieceProcessor(PALIGEMMA_TOKENIZER_PATH)\n",
        "paligemma_tokenizer = paligemma_tools.PaliGemmaTokenizer(tokenizer)\n",
        "\n",
        "# Load params - this can take up to 1 minute in T4 colabs.\n",
        "params = paligemma.load(None, PALIGEMMA_MODEL_PATH, model_config)\n",
        "\n",
        "# Define `decode` function to sample outputs from the model.\n",
        "decode_fn = predict_fns.get_all(model)['decode']\n",
        "decode = functools.partial(decode_fn, devices=jax.devices(), eos_token=tokenizer.eos_id())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxWXUO-O8zfJ"
      },
      "source": [
        "This section sets up the model for subsequent partial fine-tuning. PaLIGemma has two input modalities -- **image** and **text**. Let's look at model's parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "C1-P6DicQ02O"
      },
      "outputs": [],
      "source": [
        "# @title Print parameters of the model.\n",
        "def parameter_overview(params):\n",
        "  for path, arr in big_vision.utils.tree_flatten_with_names(params)[0]:\n",
        "    print(f\"{path:80s} {str(arr.shape):22s} {arr.dtype}\")\n",
        "\n",
        "print(\" == Model params == \")\n",
        "parameter_overview(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaCqeiBTCdm2"
      },
      "source": [
        "It is a common practice to target attention matrices $Q, K, V, O$ for fine-tuning [link](https://medium.com/@danushidk507/fine-tuning-with-lora-and-qlora-enhancing-efficiency-in-neural-network-adaptation-8b4d1473274b). You can choose to train those parameters only in Gemma (language) part or in SigLIP (image) and Gemma (language) together.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "crRUw-20wy4F"
      },
      "outputs": [],
      "source": [
        "# @title Pick trainable parameters\n",
        "#\n",
        "# To keep HBM usage low and fit in a T4 GPU (16GB HBM) we opt to only finetune\n",
        "# a part of the parameters. Additionally we keep the frozen params in float16\n",
        "# and cast trainable to float32.\n",
        "\n",
        "# Create a pytree mask of the trainable params.\n",
        "def is_trainable_param_image_language(name, param):  # pylint: disable=unused-argument\n",
        "  if name.startswith(\"img/Transformer/encoderblock/MultiHeadDotProductAttention_0\"):  return True\n",
        "  if name.startswith('llm/layers/attn'):    return True\n",
        "  if name.startswith(\"llm/\"):              return False\n",
        "  if name.startswith(\"img/\"):              return False\n",
        "  raise ValueError(f\"Unexpected param name {name}\")\n",
        "\n",
        "def is_trainable_param_language(name, param):  # pylint: disable=unused-argument\n",
        "  if name.startswith(\"llm/layers/attn\"):  return True\n",
        "  if name.startswith(\"llm/\"):              return False\n",
        "  if name.startswith(\"img/\"):              return False\n",
        "  raise ValueError(f\"Unexpected param name {name}\")\n",
        "\n",
        "finetuning = \"image+language\" # @param [\"image+language\", \"language\"]\n",
        "is_trainable_param = is_trainable_param_language if finetuning == \"language\" else is_trainable_param_image_language\n",
        "trainable_mask = big_vision.utils.tree_map_with_names(is_trainable_param, params)\n",
        "\n",
        "# If more than one device is available (e.g. multiple GPUs) the parameters can\n",
        "# be sharded across them to reduce HBM usage per device.\n",
        "mesh = jax.sharding.Mesh(jax.devices(), (\"data\"))\n",
        "\n",
        "data_sharding = jax.sharding.NamedSharding(\n",
        "    mesh, jax.sharding.PartitionSpec(\"data\"))\n",
        "\n",
        "params_sharding = big_vision.sharding.infer_sharding(\n",
        "    params, strategy=[('.*', 'fsdp(axis=\"data\")')], mesh=mesh)\n",
        "\n",
        "# Yes: Some donated buffers are not usable.\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\", message=\"Some donated buffers were not usable\")\n",
        "\n",
        "@functools.partial(jax.jit, donate_argnums=(0,), static_argnums=(1,))\n",
        "def maybe_cast_to_f32(params, trainable):\n",
        "  # Cast others to float16, since some GPUs don't support bf16.\n",
        "  return jax.tree.map(lambda p, m: p.astype(jnp.float32)\n",
        "                      if m else p.astype(jnp.float16),\n",
        "                      params, trainable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yu6nepcHzMF"
      },
      "source": [
        "Let's take a look at the parameter mask for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c1Xw8yk_XVQg"
      },
      "outputs": [],
      "source": [
        "# @title Trainable parameters mask.\n",
        "trainable_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "d9Z5a6GSwmq9"
      },
      "outputs": [],
      "source": [
        "# @title Move params to GPU/TPU memory.\n",
        "\n",
        "# Loading all params in simultaneous - albeit much faster and more succinct -\n",
        "# requires more RAM than the T4 colab runtimes have by default (12GB RAM).\n",
        "# Instead we do it param by param.\n",
        "params, treedef = jax.tree.flatten(params)\n",
        "sharding_leaves = jax.tree.leaves(params_sharding)\n",
        "trainable_leaves = jax.tree.leaves(trainable_mask)\n",
        "for idx, (sharding, trainable) in enumerate(zip(sharding_leaves, trainable_leaves)):\n",
        "  #params[idx] = big_vision.utils.reshard(params[idx], sharding)\n",
        "  params[idx] = maybe_cast_to_f32(params[idx], False) # trainable\n",
        "  params[idx].block_until_ready()\n",
        "params = jax.tree.unflatten(treedef, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BK1Jf8LWpN3W"
      },
      "outputs": [],
      "source": [
        "# @title Define the Evaluation/Inference loop.\n",
        "def make_predictions(data_iterator,\n",
        "                     postprocess_tokens,\n",
        "                     num_examples=None,\n",
        "                     batch_size=4, seqlen=paligemma_gesture_preparation._SEQLEN,\n",
        "                     sampler=\"greedy\"):\n",
        "  num_predicted = 0\n",
        "  while True:\n",
        "    # Construct a list of examples in the batch.\n",
        "    examples = []\n",
        "    try:\n",
        "      for _ in range(batch_size):\n",
        "        examples.append(next(data_iterator))\n",
        "        examples[-1][\"_mask\"] = np.array(True)  # Indicates true example.\n",
        "    except StopIteration:\n",
        "      if len(examples) == 0:\n",
        "        break\n",
        "\n",
        "    # Not enough examples to complete a batch. Pad by repeating last example.\n",
        "    while len(examples) % batch_size:\n",
        "      examples.append(dict(examples[-1]))\n",
        "      examples[-1][\"_mask\"] = np.array(False)  # Indicates padding example.\n",
        "\n",
        "    # Convert list of examples into a dict of np.arrays and load onto devices.\n",
        "    batch = jax.tree.map(lambda *x: np.stack(x), *examples)\n",
        "    batch = big_vision.utils.reshard(batch, data_sharding)\n",
        "\n",
        "    # Make model predictions\n",
        "    tokens = decode({\"params\": params}, batch=batch,\n",
        "                    max_decode_len=seqlen, sampler=sampler)\n",
        "\n",
        "    # Fetch model predictions to device and detokenize.\n",
        "    tokens, mask = jax.device_get((tokens, batch[\"_mask\"]))\n",
        "    tokens = tokens[mask]  # remove padding examples.\n",
        "    responses = [postprocess_tokens(t) for t in tokens]\n",
        "\n",
        "    if num_examples and num_predicted + len(responses) \u003e= num_examples:\n",
        "      responses = responses[:num_examples - num_predicted]\n",
        "      yield from responses\n",
        "      break\n",
        "    yield from responses\n",
        "    num_predicted += len(responses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIIeqROKZnHG"
      },
      "source": [
        "## Apply the model on the validation dataset\n",
        "\n",
        "Now that the model is loaded in memory, let's apply it on the validation dataset and see what it does. The next few sections define some helper functions to load the data, pass it to the model and visualize the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOqRPLCk3hLl"
      },
      "outputs": [],
      "source": [
        "def valid_data_iterator(examples_to_display):\n",
        "  for _, example in examples_to_display:\n",
        "    yield paligemma_gesture_preparation.prepare_inference_input(\n",
        "        paligemma_tokenizer, example['image']\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjmIX294lpHF"
      },
      "outputs": [],
      "source": [
        "examples_to_display = [\n",
        "    (k, v)\n",
        "    for k, v in valid_examples.items()\n",
        "    if not v['label'].startswith('instruct') and not v['label'].startswith('question')\n",
        "]\n",
        "random.seed(123)\n",
        "random.shuffle(examples_to_display)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLVr11bGrZHU"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "valid_data_iter = valid_data_iterator(examples_to_display)\n",
        "for pred in tqdm(make_predictions(valid_data_iter,\n",
        "                     postprocess_tokens=paligemma_tokenizer.postprocess_tokens,\n",
        "                     num_examples=16)):\n",
        "  predictions.append(data_processing.DocumentEditingLabel.from_string(pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21AhMQ19mK61"
      },
      "outputs": [],
      "source": [
        "correct_class = []\n",
        "\n",
        "for (page_id, example), prediction in zip(examples_to_display, predictions):\n",
        "  if prediction is None:\n",
        "    continue\n",
        "  correct_class.append(prediction.gesture == example['classname'])\n",
        "\n",
        "print(f'Gesture classification accuracy: {np.mean(correct_class)}')\n",
        "for i, pred in enumerate(predictions):\n",
        "  print('prediction:', pred)\n",
        "  print('target:    ', examples_to_display[i][1]['label'])\n",
        "  print('-----------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRrD1KVhTN8Q"
      },
      "source": [
        "## Visualize the predictions on the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5ihHq6OtnLE8"
      },
      "outputs": [],
      "source": [
        "# @title Define visualization helper functions.\n",
        "\n",
        "def format_label(prefix: str, label: str, color: str) -\u003e str:\n",
        "  parts = label.split(' ')\n",
        "  class_name = parts[0]\n",
        "  bbox = parts[1:5]\n",
        "  text = parts[5:]\n",
        "  return (\n",
        "      f'\u003ctt\u003e{prefix} '\n",
        "      f'\u003cb\u003e{class_name}\u003c/b\u003e \u003cspan'\n",
        "      f' style=\"background-color:{color}\"\u003e{\" \".join(bbox)}\u003c/span\u003e'\n",
        "      f' \u003cspan\u003e{\" \".join(text)}\u003c/span\u003e\u003c/tt\u003e'\n",
        "  )\n",
        "\n",
        "def format_prediction(prefix: str, prediction: data_processing.DocumentEditingLabel, color: str) -\u003e str:\n",
        "  return (\n",
        "      f'\u003ctt\u003e{prefix} '\n",
        "      f'\u003cb\u003e{prediction.gesture}\u003c/b\u003e \u003cspan'\n",
        "      f' style=\"background-color:{color}\"\u003e{prediction.bbox.top:.0f} '\n",
        "      f'{prediction.bbox.left:.0f} {prediction.bbox.bottom:.0f} '\n",
        "      f'{prediction.bbox.right:.0f}\u003c/span\u003e'\n",
        "      f' \u003cspan\u003e{prediction.text}\u003c/span\u003e\u003c/tt\u003e'\n",
        "  )\n",
        "\n",
        "def table_row(contents: list[list[str]]) -\u003e str:\n",
        "  cell_contents = ['\u003cbr/\u003e'.join(content) for content in contents]\n",
        "  cells = [\n",
        "      '\u003ctd style=\"border: 1px solid lightgray; text-align:'\n",
        "      f' center\"\u003e{cell_content}\u003c/td\u003e'\n",
        "      for cell_content in cell_contents\n",
        "  ]\n",
        "  return f'\u003ctr\u003e{\"\".join(cells)}\u003c/tr\u003e'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVFfqQr2aHWr"
      },
      "outputs": [],
      "source": [
        "html = [\"\"\"\n",
        "\u003ctable style=\"width: 1500px; border-collapse: collapse; border: 1px solid lightgray;\"\u003e\n",
        "\u003cthead\u003e\n",
        "  \u003ctr\u003e\n",
        "    \u003cth\u003ePage\u003c/th\u003e\n",
        "    \u003cth\u003eModel output\u003c/th\u003e\n",
        "    \u003cth\u003eDocument before\u003c/th\u003e\n",
        "    \u003cth\u003eDocument after\u003c/th\u003e\n",
        "  \u003c/tr\u003e\n",
        "\u003c/thead\u003e\n",
        "\u003ctbody\u003e\n",
        "\"\"\"]\n",
        "\n",
        "for (_, example), prediction_full in tqdm(zip(examples_to_display, predictions)):\n",
        "  if prediction_full is None:\n",
        "    continue\n",
        "\n",
        "  html_row_contents = []\n",
        "  page = valid_pages[example['page_id']]\n",
        "\n",
        "  # Display the page ID with a rendering of the gesture.\n",
        "  html_row_contents.append([\n",
        "      f'\u003ch4\u003e{page_id}\u003c/h4\u003e',\n",
        "      rendering.to_html_image(example['original_image'], width=300),\n",
        "  ])\n",
        "\n",
        "  # Ground truth and prediction strings.\n",
        "  html_row_contents.append([\n",
        "      format_label('Label:', example['label'], 'rgba(0, 255, 0, 0.3)'),\n",
        "      format_prediction('Pred :', prediction_full, 'rgba(255, 0, 255, 0.3)'),\n",
        "  ])\n",
        "\n",
        "  # Document before and after the edit.\n",
        "  page_copy = copy.deepcopy(page)\n",
        "\n",
        "  bbox = example['bbox']\n",
        "  composition_bbox = example['composition_bbox']\n",
        "  writing_guide_bbox = example['writing_guide']\n",
        "\n",
        "  bbox = rendering.bbox_to_image_space(bbox, composition_bbox)\n",
        "\n",
        "  prediction = prediction_full.bbox\n",
        "  prediction = rendering.bbox_to_image_space(prediction, composition_bbox)\n",
        "  prediction_classname = prediction_full.gesture\n",
        "  prediction_text = prediction_full.text\n",
        "\n",
        "  page_copy.edit(\n",
        "      edit_name=prediction_classname,\n",
        "      edit_bbox=prediction,\n",
        "      text=prediction_text,\n",
        "  )\n",
        "\n",
        "  # We compute first the \"after\" state, which returns an area of interest we can\n",
        "  # crop around for better readability in the output table.\n",
        "  rendering_after = rendering.render_document(\n",
        "      page_copy,\n",
        "      overlay_bboxes={'': composition_bbox},\n",
        "      crop_area=True,\n",
        "  )\n",
        "  html_image_after = rendering.to_html_image(rendering_after.image, width=400)\n",
        "\n",
        "  rendering_before = rendering.render_document(\n",
        "      page,\n",
        "      overlay_bboxes={'lime': bbox, 'fuchsia': prediction},\n",
        "      ink=ink_by_hash[example['ink_hash']],\n",
        "      crop_area=rendering_after.area_of_interest,\n",
        "  )\n",
        "  html_image_before = rendering.to_html_image(rendering_before.image, width=400)\n",
        "\n",
        "  html_row_contents.append([html_image_before])\n",
        "  html_row_contents.append([html_image_after])\n",
        "  html.append(table_row(html_row_contents))\n",
        "\n",
        "html.append('\u003c/tbody\u003e\u003c/table\u003e')\n",
        "display(HTML(''.join(html)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-9JG2yJnopD"
      },
      "source": [
        "## Interactive Ink Canvas\n",
        "In this section you can load one document from the dataset and play with the model. Draw a gesture with the mouse and click the `interpret` button to make a model call. If everything goes well, you should be able to see on the right the image provided as input to the model and, overlaid on top of it, the detected target.\n",
        "\n",
        "\n",
        "✅ Feel free to come back to this part throughout the colab (in particular in later stages after fine-tuning to experiment with the predictions of the model).\n",
        "\n",
        "**After fine-tuning:** Parameter margin is used for cropping the image around the ink. You probably want to keep it the same as in synthetic data generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3Kvn-6FRl9gq"
      },
      "outputs": [],
      "source": [
        "# @title Define prediction helper function.\n",
        "\n",
        "margin = 40 #@param {type: \"integer\"}\n",
        "def canvas_predict_fn(ink: document_editing.Ink, image: Image.Image):\n",
        "  # Prepare a square area around the gesture.\n",
        "  rendered_ink = rendering.render_ink_on_image(ink, image, add_semi_transparent_overlay=True)\n",
        "\n",
        "  ink_bbox = ink.get_bbox()\n",
        "  size = max(ink_bbox.width, ink_bbox.height) + margin\n",
        "  gesture_area = document_editing.BoundingBox(\n",
        "      top=ink_bbox.center.y - size // 2,\n",
        "      left=ink_bbox.center.x - size // 2,\n",
        "      bottom=ink_bbox.center.y + size // 2,\n",
        "      right=ink_bbox.center.x + size // 2,\n",
        "  )\n",
        "  model_input = rendered_ink.crop((\n",
        "      gesture_area.left,\n",
        "      gesture_area.top,\n",
        "      gesture_area.right,\n",
        "      gesture_area.bottom,\n",
        "  )).resize((_IMAGE_RESOLUTION, _IMAGE_RESOLUTION))\n",
        "\n",
        "  # Prepare the input for the model.\n",
        "  composition = Image.new(\"RGBA\", model_input.size, \"white\")\n",
        "  composition.paste(model_input, mask=model_input)\n",
        "  inference_input = paligemma_gesture_preparation.prepare_inference_input(\n",
        "      paligemma_tokenizer, image=composition\n",
        "  )\n",
        "\n",
        "  notebook_canvas.set_debug_output('', rendering.to_data_url(composition))\n",
        "  prediction = next(\n",
        "      make_predictions(\n",
        "          iter([inference_input]),\n",
        "          postprocess_tokens=paligemma_tokenizer.postprocess_tokens,\n",
        "          batch_size=1,\n",
        "          num_examples=1,\n",
        "      )\n",
        "  )\n",
        "  parsed_prediction = data_processing.DocumentEditingLabel.from_output(\n",
        "      prediction, loc_tokens=True\n",
        "  )\n",
        "  if not parsed_prediction:\n",
        "    notebook_canvas.set_debug_output(f'❌ (could not parse) {prediction}', '')\n",
        "    return data_processing.DocumentEditingLabel(\n",
        "        gesture='none',\n",
        "        bbox=document_editing.BoundingBox(top=0,left=0, bottom=0, right=0),\n",
        "        text=''\n",
        "    )\n",
        "\n",
        "  scale = _IMAGE_RESOLUTION / paligemma_tools.LOCATION_TOKENS_RANGE_MAX\n",
        "\n",
        "  # Show the predicted bounding box as an overlay to the input composition.\n",
        "  draw = ImageDraw.Draw(composition)\n",
        "  if parsed_prediction.gesture == 'point':\n",
        "    draw.circle(\n",
        "        (\n",
        "            parsed_prediction.bbox.left * scale,\n",
        "            parsed_prediction.bbox.top * scale\n",
        "        ),\n",
        "        fill='fuchsia',\n",
        "        radius=8\n",
        "    )\n",
        "    draw.circle(\n",
        "        (\n",
        "            parsed_prediction.bbox.right * scale,\n",
        "            parsed_prediction.bbox.bottom * scale\n",
        "        ),\n",
        "        fill='fuchsia',\n",
        "        radius=8\n",
        "    )\n",
        "  else:\n",
        "    draw.rectangle(\n",
        "        [\n",
        "            (\n",
        "                parsed_prediction.bbox.left * scale,\n",
        "                parsed_prediction.bbox.top * scale,\n",
        "            ),\n",
        "            (\n",
        "                parsed_prediction.bbox.right * scale,\n",
        "                parsed_prediction.bbox.bottom * scale,\n",
        "            ),\n",
        "        ],\n",
        "        outline=\"fuchsia\",\n",
        "        width=2,\n",
        "    )\n",
        "\n",
        "  # Show a view of the composition with the predicted bounding box on the side panel.\n",
        "  composition_image_url = rendering.to_data_url(composition)\n",
        "  notebook_canvas.set_debug_output('✅ ' + parsed_prediction.to_string(data_processing.BBOX_FORMAT), composition_image_url)\n",
        "\n",
        "  # Convert the predicted bounding box back to image space.\n",
        "  parsed_prediction.bbox = rendering.bbox_to_image_space(\n",
        "      parsed_prediction.bbox, gesture_area\n",
        "  )\n",
        "\n",
        "  return parsed_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECuE7CyGxCmY"
      },
      "outputs": [],
      "source": [
        "one_page = document_editing.load_page(PAGES_DIR, '9790964376811024979')\n",
        "\n",
        "# This makes the word bounding box align more closely to the matplotlib\n",
        "# rendering used by the colab as opposed to the original element sizes in the\n",
        "# webpage renderings.\n",
        "one_page.tighten_bboxes_for_colab_canvas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQkPVkOx2JA1"
      },
      "outputs": [],
      "source": [
        "canvas = notebook_canvas.Canvas(one_page, canvas_predict_fn, canvas_max_width=800, canvas_max_height=1600)\n",
        "canvas.display_interaction_widget()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQZxmkiPuxOn"
      },
      "source": [
        "# Section 2 - Few-shot with Gemini\n",
        "\n",
        "In this part, we will focus on trying to use bigger Foundational Models (e.g. Gemini 2.0) to solve the document editing task through 0-shot/few-shot approaches. The reason why we are not re-using a model from the PaLiGemma family is because these models haven't been trained for instruction-following and are therefore unlikely to work well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd4bHZ-2vNuM"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "We will load the training and validation dataset for document editing. We will use the training set to sample the different few-shot examples, and run the evaluation on the validation dataset. The few-shot examples are sampled in a stratified way (where stratas are defined as the gesture types), to ensure the model sees examples of each of the classes.\n",
        "\n",
        "⏩ You do not need to interact with the code in the following cells, you may therefore keep them collapsed to keep the colab as readable as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fYTsPTFvPCB"
      },
      "outputs": [],
      "source": [
        "# Validation dataset\n",
        "eval_samples = valid_examples.values()\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    {\n",
        "      'ink_hash': np.array([example[\"ink_hash\"] for example in eval_samples]),\n",
        "      'image/encoded': np.array([example[\"image\"] for example in eval_samples]),\n",
        "      'label': np.array([example[\"label\"] for example in eval_samples]),\n",
        "      'image_width': np.array([example[\"original_image\"].width for example in eval_samples]),\n",
        "      'image_height': np.array([example[\"original_image\"].height for example in eval_samples]),\n",
        "    }\n",
        ")\n",
        "\n",
        "# Train dataset\n",
        "train_samples = train_examples.values()\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    {\n",
        "      'ink_hash': np.array([example[\"ink_hash\"] for example in train_samples]),\n",
        "      'image/encoded': np.array([example[\"image\"] for example in train_samples]),\n",
        "      'label': np.array([example[\"label\"] for example in train_samples]),\n",
        "      'image_width': np.array([example[\"original_image\"].width for example in train_samples]),\n",
        "      'image_height': np.array([example[\"original_image\"].height for example in train_samples]),\n",
        "    }\n",
        ")\n",
        "\n",
        "def get_gesture(example):\n",
        "  return tf.strings.split(example[\"label\"], \" \", 1)[0]\n",
        "\n",
        "def get_normalization_factor(example):\n",
        "  return (example['image_width'] / 1000, example['image_height'] / 1000)\n",
        "\n",
        "\n",
        "shot_sampler = sampling.StratifiedSampler(train_dataset, get_gesture)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_8sOrGmvSqy"
      },
      "source": [
        "## Model Loading\n",
        "\n",
        "This cell loads the Gemini Model API Client and the corresponding inference function to be used later on to infer the prediction for a given document gesture.\n",
        "\n",
        "✅ Please add a Gemini API Key through your secrets manager under the \"🔑\" in the left panel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdXYIX98vVY_"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "MODEL_NAME = \"models/gemini-2.0-flash\"\n",
        "\n",
        "client = model_api.get_client(API_KEY)\n",
        "inference_fn = model_api.client_to_inference_fn(client, MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qcVfxLwvwiu"
      },
      "source": [
        "## Few Shot Prompt Definition\n",
        "\n",
        "In this section, we focus on defining the prompt format that will be used for querying Gemini in 0-shot/few-shot settings. We provide some helper classes, functions and overall template for defining the prompt, but feel free to play around with them in different manners to reach the best possible results!\n",
        "\n",
        "The provided template is composed of 3 main parts:\n",
        "- The `PROMPT` variable which corresponds to the main instruction given to the model.\n",
        "- The `shot_prefix` variable which will add text to the prefix if there is at least 1 shot example provided.\n",
        "- The `GestureFewShotPrompter.prepare_example` function which defines the format into which examples are to be generated to be added to the prompt (when `is_shot` is `True`) and used for formatting the current inference example (when `is_shot` is `False`).\n",
        "\n",
        "\n",
        "Note: Gemini normalizes the coordinate system to be `([0, 1000], [0, 1000])` instead of `([0, width], [0, height])` and prefers being prompted with y coordinate first and x coordinate second.\n",
        "\n",
        "⏩ Feel free to skip the content of these cells, since we only define helper methods therein."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "M1t_kAqrvZtd"
      },
      "outputs": [],
      "source": [
        "#@title Helper class\n",
        "GESTURES = (\n",
        "    'crop',\n",
        "    'delete',\n",
        "    'insert',\n",
        "    'instruct_image',\n",
        "    'instruct_text',\n",
        "    'question',\n",
        "    'select',\n",
        "    'underline',\n",
        ")\n",
        "\n",
        "class GestureFewShotPrompter(in_context_learning.FewShotPrompter):\n",
        "  def prepare_example(self, example, is_shot):\n",
        "    data = []\n",
        "\n",
        "    image = self.load_image(example[\"image/encoded\"])\n",
        "    data.append(image)\n",
        "\n",
        "    if is_shot:\n",
        "      label = example[\"label\"].decode()\n",
        "      data.append(label)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLAQdZBOccHz"
      },
      "source": [
        "## Prompt definition\n",
        "✅ Please expand and the following cell to play around with the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZJ0MYbjcRMg"
      },
      "outputs": [],
      "source": [
        "PROMPT = (\n",
        "f\"\"\"You receive as input an image that contains some text and a human gesture in red ink strokes.\n",
        "\n",
        "Your task is to predict the type of gesture, the bounding box of the text it is annotating with a transcription of what the user wrote in handwriting (if anything was written) in the following format \u003cgesture\u003e \u003cymin\u003e \u003cxmin\u003e \u003cymax\u003e \u003cxmax\u003e [\u003ctranscription\u003e].\n",
        "The bounding box should have normalized coordinates as int [0, 1000). (0, 0) is the top left corner and the y, x coordinate values are relative values with respect to image height and width. Only output the gesture type, bounding box and text (if present) and nothing else.\n",
        "\n",
        "The possible gestures are:\n",
        "{os.linesep.join(f\"- {gesture}\" for gesture in GESTURES)}\n",
        "\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-0zjkVtov04-"
      },
      "outputs": [],
      "source": [
        "#@title Preview entire input to the model\n",
        "shot_prefix = \"\"\"Now, we show you some examples for each gesture type.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "example = next(iter(valid_dataset.as_numpy_iterator()))\n",
        "n_shots = 2 # @param\n",
        "shots = list(shot_sampler.sample(num_examples=n_shots))\n",
        "shots_gemini = [\n",
        "    data_processing.transform_example_label(\n",
        "        example,\n",
        "        data_processing.BBOX_PATTERN,\n",
        "        data_processing.BBOX_FORMAT,\n",
        "        get_normalization_factor(example)\n",
        "    )\n",
        "    for example in shots\n",
        "]\n",
        "prompter = GestureFewShotPrompter(PROMPT, shot_prefix, shots_gemini)\n",
        "prompter.display_prompt(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Jg4AfSBv3Kr"
      },
      "source": [
        "## Fewshot inference\n",
        "\n",
        "Based on the prompt you defined above, the following cells will run inference on a different number of shots. Feel free to modify your prompt to try and reach the best results!\n",
        "\n",
        "**Note**: free AI Studio API Keys are limited to 15 Requests per model per minute, and 1500 Request per model per day, and the following cell will consume 800 of those requests!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFXRJ_u1v5Iu"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "confusion_matrix = {}\n",
        "\n",
        "N_SHOTS = [0, 1, 2, 4, 8]\n",
        "\n",
        "for n_shots in N_SHOTS:\n",
        "  shots = list(shot_sampler.sample(num_examples=n_shots))\n",
        "  shots_gemini = [\n",
        "      data_processing.transform_example_label(\n",
        "          example,\n",
        "          data_processing.BBOX_PATTERN,\n",
        "          data_processing.BBOX_FORMAT,\n",
        "          get_normalization_factor(example)\n",
        "      )\n",
        "      for example in shots\n",
        "  ]\n",
        "  prompter = GestureFewShotPrompter(PROMPT, shot_prefix, shots_gemini)\n",
        "  targets, predictions = in_context_learning.infer_fewshot(\n",
        "      inference_fn,\n",
        "      prompter,\n",
        "      valid_dataset,\n",
        "      normalize_fn=get_normalization_factor\n",
        "  )\n",
        "\n",
        "  accuracies, ious, cers = metrics.compute_document_editing_metrics(targets, predictions)\n",
        "  cm = metrics.confusion_matrix(predictions, targets)\n",
        "\n",
        "  results[n_shots] = {\n",
        "      'Accuracy': sum(accuracies) / len(accuracies),\n",
        "      'IoU': sum(ious) / len(ious),\n",
        "      'CER': sum(cers) / len(cers),\n",
        "  }\n",
        "  confusion_matrix[n_shots] = cm\n",
        "\n",
        "metrics.plot_fewshot_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07N3o_ha0hEM"
      },
      "source": [
        "# Section 3 - Extending the model\n",
        "\n",
        "In this section you will attempt to define a brand new gesture class and further finetune the existing model for recognizing it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju3FvYNlgWas"
      },
      "source": [
        "## Synthetic dataset generation\n",
        "The trained model classifies and localizes a fixed set of classes. **What if we want to add a new class?**\n",
        "\n",
        "In this section we will show how to generate additional data with new class – **arrow**. As a first step we need a source of arrow inks. We use the [MathWriting](https://github.com/google-research/google-research/tree/master/mathwriting) dataset, that contains handwritten math formulas and extract LaTeX symbols\n",
        "\n",
        "$$\\leftrightarrow, \\Leftrightarrow$$\n",
        "\n",
        "Let's load the dataset (may take around 5 minutes).\n",
        "\n",
        "**Important:** if you've restarted the runtime and previously generated the dataset, you don't need to rerun this section. The data should be located in the directory `data/mathwriting-2024/arrow_dataset`\n",
        "\n",
        "✅ You can modify dataset generation parameters, such as the margin around an arrow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9depvslzdw4"
      },
      "source": [
        "### Create dataset on the spot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQuwtoOn3V0G"
      },
      "outputs": [],
      "source": [
        "# @title Load MathWriting dataset.\n",
        "\n",
        "MATHWRITING_FILE_NAME = 'mathwriting-2024.tgz'\n",
        "MATHWRITING_BASE_PATH = \"data/mathwriting-2024\"\n",
        "MATHWRITING_FILE_PATH = os.path.join(MATHWRITING_BASE_PATH, MATHWRITING_FILE_NAME)\n",
        "\n",
        "if not os.path.exists(MATHWRITING_BASE_PATH):\n",
        "  !mkdir -p {MATHWRITING_BASE_PATH}\n",
        "\n",
        "!wget -nc https://storage.googleapis.com/mathwriting_data/mathwriting-2024.tgz --no-check-certificate -O {MATHWRITING_FILE_PATH}\n",
        "shutil.unpack_archive(MATHWRITING_FILE_PATH, 'data/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YfyB4uYD54fW"
      },
      "outputs": [],
      "source": [
        "# @title Extract arrows.\n",
        "\n",
        "def get_symbol_ink(symbol: arrow_ink_tools.InkPart) -\u003e document_editing.Ink:\n",
        "  \"\"\"Computes the actual ink from an InkPart object.\"\"\"\n",
        "  ink = arrow_ink_tools.read_inkml_file(\n",
        "      os.path.join(MATHWRITING_BASE_PATH, \"train\", f\"{symbol.source_sample_id}.inkml\"))\n",
        "  strokes = [ink.strokes[i] for i in symbol.stroke_indices]\n",
        "  return document_editing.Ink(strokes=strokes)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_ink(ax: plt.Axes, ink: document_editing.Ink):\n",
        "  \"\"\"Plots the ink data on the given axes.\"\"\"\n",
        "  for stroke in ink.strokes:\n",
        "    ax.plot(stroke.xs, stroke.ys, color=\"red\", linewidth=2, zorder=100)\n",
        "\n",
        "symbols = arrow_ink_tools.read_symbols_file(os.path.join(MATHWRITING_BASE_PATH, 'symbols.jsonl'))\n",
        "arrows = [s for s in symbols if s.label in {'\\\\leftrightarrow', '\\\\Leftrightarrow'}]\n",
        "print(f\"We've extracted {len(arrows)} unique arrows from MathWriting dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4VP2c4shb5D"
      },
      "source": [
        "For each arrow we find **left and right critical points** – edges of an arrow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCApdvridgdP"
      },
      "outputs": [],
      "source": [
        "arrows_downloaded = []\n",
        "for i, arrow in enumerate(arrows):\n",
        "  arrow_ink = arrow_ink_tools.ArrowInk(ink=get_symbol_ink(arrow))\n",
        "  arrows_downloaded.append(arrow_ink)\n",
        "  if i == 0:\n",
        "    plt.close()\n",
        "    plot_ink(ink=arrow_ink.ink, ax=plt)\n",
        "    plt.plot(arrow_ink.left_critical_point.x, arrow_ink.left_critical_point.y, 'bo')\n",
        "    plt.plot(arrow_ink.right_critical_point.x, arrow_ink.right_critical_point.y, 'bo')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dnoC1s91LEFa"
      },
      "outputs": [],
      "source": [
        "# @title Helper functions for sampling words and arrows.\n",
        "def find_random_word_pair(page: document_editing.Page) -\u003e tuple[int, int]:\n",
        "  \"\"\"Finds two random words that are close on the page.\"\"\"\n",
        "  word_ids = []\n",
        "  for (id_, element) in page.element_from_id.items():\n",
        "    if element.class_name == 'word':\n",
        "      word_ids.append(id_)\n",
        "  page_reader = rendering.render_document(page)\n",
        "\n",
        "  possible_pairs = []\n",
        "  for i in range(len(word_ids)):\n",
        "    for j in range(i + 1, len(word_ids)):\n",
        "      c1 = page_example.element_from_id[word_ids[i]].bbox.center\n",
        "      x1, y1 = c1.x, c1.y\n",
        "      c2 = page_example.element_from_id[word_ids[j]].bbox.center\n",
        "      x2, y2 = c2.x, c2.y\n",
        "\n",
        "      y_size = page_reader.extent.bottom - page_reader.extent.top\n",
        "      x_size = page_reader.extent.right - page_reader.extent.left\n",
        "      share_x = abs((x2 - x1) / x_size)\n",
        "      share_y = abs((y2 - y1) / y_size)\n",
        "\n",
        "      if share_x \u003c= 0.1 and share_y \u003c= 0.1:\n",
        "        possible_pairs.append((word_ids[i], word_ids[j]))\n",
        "\n",
        "  random_pair = random.choice(possible_pairs)\n",
        "  y1 = page_example.element_from_id[random_pair[0]].bbox.center.y\n",
        "  y2 = page_example.element_from_id[random_pair[1]].bbox.center.y\n",
        "\n",
        "  if y1 \u003e= y2:\n",
        "    return (random_pair[1], random_pair[0])\n",
        "\n",
        "  return random_pair\n",
        "\n",
        "def sample_two_words_and_fit_an_arrow(page_example):\n",
        "  page_with_tight_bboxes = copy.deepcopy(page_example)\n",
        "  page_with_tight_bboxes.tighten_bboxes_for_colab_canvas()\n",
        "\n",
        "  word_id1, word_id2 = find_random_word_pair(page_with_tight_bboxes)\n",
        "  arrow = random.sample(arrows_downloaded, 1)[0]\n",
        "  arrow_fitter = arrow_ink_tools.ArrowPageFitter(page_with_tight_bboxes, word_id1, word_id2)\n",
        "  return arrow_fitter, arrow_fitter.fit_to_page(arrow=arrow, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyb_Q8hCNgpl"
      },
      "source": [
        "Our next step is to choose two random words on the page that are relatively close together. We then fit an arrow between them by aligning one of its endpoints with the center of the first word, and then rotating and scaling the arrow until its other endpoint aligns with the center of the second word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNM18HL-vJFL"
      },
      "outputs": [],
      "source": [
        "dataset = []\n",
        "num_tries = 3\n",
        "for valid_page_id in tqdm(valid_page_ids):\n",
        "  page_example = valid_pages[valid_page_id]\n",
        "  for _ in range(num_tries):\n",
        "    arrow_fitter, final_arrow = sample_two_words_and_fit_an_arrow(page_example)\n",
        "    if arrow_ink_tools.check_that_arrow_is_located_correctly(arrow_fitter, final_arrow):\n",
        "      dataset.append((arrow_fitter, final_arrow))\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TbMi1yKNzgI"
      },
      "source": [
        "Next, we crop images around each arrow (with a margin) and save them together with the targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McYWYlnpOwwa"
      },
      "outputs": [],
      "source": [
        "# @title Save images on disk and prepare targets.\n",
        "GENERATED_DATASET_PATH = os.path.join(MATHWRITING_BASE_PATH, 'arrow_dataset')\n",
        "if not os.path.exists(GENERATED_DATASET_PATH):\n",
        "  ! mkdir {GENERATED_DATASET_PATH}\n",
        "\n",
        "margin = 40 #@param {type: \"integer\"}\n",
        "prepared_targets = []\n",
        "for i, (arrow_fitter, final_arrow) in tqdm(enumerate(dataset)):\n",
        "  center = final_arrow.ink.get_bbox().center\n",
        "  x, y = center.x, center.y\n",
        "  size = max(final_arrow.ink.get_bbox().width, final_arrow.ink.get_bbox().height) + margin\n",
        "  crop_area = document_editing.BoundingBox(y - size // 2, x - size // 2, y + size // 2, x + size // 2)\n",
        "  image = arrow_fitter.page.image_from_id[-1]\n",
        "  image_with_ink = rendering.render_ink_on_image(\n",
        "      final_arrow.ink,\n",
        "      image,\n",
        "      add_semi_transparent_overlay=True\n",
        "  )\n",
        "  cropped_image = image_with_ink.crop(\n",
        "      (crop_area.left, crop_area.top, crop_area.right, crop_area.bottom)\n",
        "  )\n",
        "  cropped_image.save(os.path.join(GENERATED_DATASET_PATH, f'{i+1}.png'))\n",
        "  prepared_targets.append(arrow_ink_tools.get_arrow_target(arrow_fitter, crop_area))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TyFaM3Bfwc5"
      },
      "outputs": [],
      "source": [
        "# @title Save jsonl files for datasets.\n",
        "data_path = os.path.join(GENERATED_DATASET_PATH, 'data_train90.jsonl')\n",
        "with open(data_path, 'w') as f:\n",
        "  for i, target in enumerate(prepared_targets[:90]):\n",
        "    json_data = {\"prefix\": \"\", \"suffix\": target, \"image\": f\"{i+1}.png\"}\n",
        "    json.dump(json_data, f)\n",
        "    f.write('\\n')\n",
        "\n",
        "data_path = os.path.join(GENERATED_DATASET_PATH, 'data_val10.jsonl')\n",
        "with open(data_path, 'w') as f:\n",
        "  for i, target in enumerate(prepared_targets[90:]):\n",
        "    json_data = {\"prefix\": \"\", \"suffix\": target, \"image\": f\"{i+1}.png\"}\n",
        "    json.dump(json_data, f)\n",
        "    f.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJzUGVhAWqpl"
      },
      "source": [
        "## Example with arrow on a page and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3Of20p5mvEe"
      },
      "outputs": [],
      "source": [
        "example_id = 5 #@param {type: \"integer\"}\n",
        "arrow_fitter, final_arrow = dataset[example_id]\n",
        "page_reader = rendering.render_document(\n",
        "      arrow_fitter.page,\n",
        "      overlay_bboxes={'lime': arrow_fitter.word_id1_bbox, 'fuchsia': arrow_fitter.word_id2_bbox,},\n",
        "      ink=final_arrow.ink)\n",
        "page_reader.image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPv3uyTWhjmh"
      },
      "outputs": [],
      "source": [
        "print(f'Example of prepared target: {prepared_targets[6]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFGPA6Q42uZf"
      },
      "source": [
        "## PaLIGemma fine-tuning on synthetic data\n",
        "\n",
        "In this section we will further fine-tune the model for it to learn the new arrow-based gesture which will swap two words on a page. In training dataset we have the following mixture:\n",
        "\n",
        "* 50% new arrow dataset\n",
        "* 50% existing training dataset with other gestures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgWstW_6f7WD"
      },
      "outputs": [],
      "source": [
        "# @title Load train and validation datasets from jsonl format.\n",
        "\n",
        "train_dataset = big_vision.datasets.jsonl.DataSource(\n",
        "    os.path.join(\"data/mathwriting-2024/arrow_dataset/data_train90.jsonl\"),\n",
        "    fopen_keys={\"image\": GENERATED_DATASET_PATH})\n",
        "\n",
        "val_dataset = big_vision.datasets.jsonl.DataSource(\n",
        "    os.path.join(\"data/mathwriting-2024/arrow_dataset/data_val10.jsonl\"),\n",
        "    fopen_keys={\"image\": GENERATED_DATASET_PATH})\n",
        "\n",
        "def original_train_data_iterator(train_examples):\n",
        "  for example in train_examples.values():\n",
        "    yield paligemma_gesture_preparation.prepare_train_input(\n",
        "        paligemma_tokenizer, image=example['image'], suffix=example['label']\n",
        "        )\n",
        "\n",
        "def train_data_iterator():\n",
        "  \"\"\"Never ending iterator over training examples.\"\"\"\n",
        "  # Shuffle examples and repeat so one can train for many epochs.\n",
        "  dataset = train_dataset.get_tfdata().shuffle(1_000).repeat()\n",
        "  original_dataset = original_train_data_iterator(train_examples)\n",
        "  for example in dataset.as_numpy_iterator():\n",
        "    image = Image.open(io.BytesIO(example['image']))\n",
        "    suffix = example['suffix'].decode()\n",
        "    yield paligemma_gesture_preparation.prepare_train_input(paligemma_tokenizer, suffix=suffix, image=image)\n",
        "    yield next(original_dataset)\n",
        "\n",
        "def val_data_iterator():\n",
        "  \"\"\"Single iterator over validation examples..\"\"\"\n",
        "  dataset = val_dataset.get_tfdata(ordered=True)\n",
        "  for example in dataset.as_numpy_iterator():\n",
        "    image = Image.open(io.BytesIO(example['image']))\n",
        "    yield paligemma_gesture_preparation.prepare_inference_input(paligemma_tokenizer, image=image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSchB_AuqEtT"
      },
      "source": [
        "We check what the model currently outputs on arrows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tvDeTTbguHl"
      },
      "outputs": [],
      "source": [
        "# @title Make predictions on arrows.\n",
        "for pred in make_predictions(val_data_iterator(),\n",
        "                     postprocess_tokens=paligemma_tokenizer.postprocess_tokens,\n",
        "                     num_examples=8):\n",
        "  print(data_processing.DocumentEditingLabel.from_string(pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MOn6Dbkxm2Z"
      },
      "source": [
        "We train the model the dataset with arrows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WU4eDQian8Nf"
      },
      "outputs": [],
      "source": [
        "# @title Define the training step.\n",
        "#\n",
        "# The main update_fn using simple SGD.\n",
        "#\n",
        "@functools.partial(jax.jit, donate_argnums=(0,))\n",
        "def update_fn(params, batch, learning_rate):\n",
        "  imgs, txts, mask_ar = batch[\"image\"], batch[\"text\"], batch[\"mask_ar\"]\n",
        "\n",
        "  def loss_fn(params):\n",
        "    text_logits, _ = model.apply({\"params\": params}, imgs, txts[:, :-1], mask_ar[:, :-1], train=True)\n",
        "    logp = jax.nn.log_softmax(text_logits, axis=-1)\n",
        "\n",
        "    # The model takes as input txts[:, :-1] but the loss is defined as predicting\n",
        "    # next tokens txts[:, 1:]. Additionally, mask_loss[:, 1:] indicates which tokens\n",
        "    # are part of the loss (e.g. prefix and padded tokens are not included).\n",
        "    mask_loss = batch[\"mask_loss\"][:, 1:]\n",
        "    targets = jax.nn.one_hot(txts[:, 1:], text_logits.shape[-1])\n",
        "\n",
        "    # Compute the loss per example. i.e. the mean of per token pplx.\n",
        "    # Since each example has a different number of tokens we normalize it.\n",
        "    token_pplx = jnp.sum(logp * targets, axis=-1)  # sum across vocab_size.\n",
        "    example_loss = -jnp.sum(token_pplx * mask_loss, axis=-1)  # sum across seq_len.\n",
        "    example_loss /= jnp.clip(jnp.sum(mask_loss, -1), 1)  # weight by num of tokens.\n",
        "\n",
        "    # batch_loss: mean of per example loss.\n",
        "    return jnp.mean(example_loss)\n",
        "\n",
        "  loss, grads = jax.value_and_grad(loss_fn)(params)\n",
        "\n",
        "  # Apply gradients to trainable params using SGD.\n",
        "  def apply_grad(param, gradient, trainable):\n",
        "    if not trainable: return param\n",
        "    return param - learning_rate * gradient\n",
        "\n",
        "  params = jax.tree_util.tree_map(apply_grad, params, grads, trainable_mask)\n",
        "\n",
        "  return params, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TQtu0ZMMximQ"
      },
      "outputs": [],
      "source": [
        "# @title Run training loop.\n",
        "#\n",
        "# Run a short training loop with cosine learning rate schedule.\n",
        "#\n",
        "# Note: the first step can be quite slow on some machines (up to several minutes)\n",
        "# due to XLA compilation of the jax.jit'd function.\n",
        "\n",
        "BATCH_SIZE = 2 # @param {type:\"integer\"}\n",
        "LEARNING_RATE = 0.0001  # @param {type:\"number\"}\n",
        "\n",
        "TRAIN_STEPS = 64 # @param {type:\"integer\"}\n",
        "EVAL_STEPS = 32\n",
        "NUM_EVAL_EXAMPLES = 4\n",
        "\n",
        "# collect valid targets\n",
        "dataset = val_dataset.get_tfdata(ordered=True)\n",
        "eval_targets = []\n",
        "for example in dataset.as_numpy_iterator():\n",
        "  eval_targets.append(example['suffix'])\n",
        "  if len(eval_targets) == NUM_EVAL_EXAMPLES:\n",
        "    break\n",
        "\n",
        "train_data_it = train_data_iterator()\n",
        "\n",
        "sched_fn = big_vision.utils.create_learning_rate_schedule(\n",
        "    total_steps=TRAIN_STEPS+1, base=LEARNING_RATE,\n",
        "    decay_type=\"cosine\", warmup_percent=0.10)\n",
        "\n",
        "for step in range(1, TRAIN_STEPS+1):\n",
        "  # Make list of N training examples.\n",
        "  examples = [next(train_data_it) for _ in range(BATCH_SIZE)]\n",
        "\n",
        "  # Convert list of examples into a dict of np.arrays and load onto devices.\n",
        "  batch = jax.tree.map(lambda *x: np.stack(x), *examples)\n",
        "  #batch = big_vision.utils.reshard(batch, data_sharding)\n",
        "\n",
        "  # Training step and report training loss\n",
        "  learning_rate = sched_fn(step)\n",
        "  params, loss = update_fn(params, batch, learning_rate)\n",
        "\n",
        "  loss = jax.device_get(loss)\n",
        "  print(f\"step: {step:2d}/{TRAIN_STEPS:2d}   lr: {learning_rate:.5f}   loss: {loss:.4f}\")\n",
        "\n",
        "  if step == 1 or (step % EVAL_STEPS) == 0:\n",
        "    print(f\"Model predictions at step {step}\")\n",
        "    for pred, target in zip(make_predictions(val_data_iterator(),\n",
        "                                 postprocess_tokens=paligemma_tokenizer.postprocess_tokens,\n",
        "                                 num_examples=NUM_EVAL_EXAMPLES), eval_targets):\n",
        "      print('predicition:', pred)\n",
        "      print('target:      ', target.decode())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJYFDGgFy9Xj"
      },
      "outputs": [],
      "source": [
        "# @title Prediction on original validation.\n",
        "predictions_after_finetune = []\n",
        "valid_data_iter = valid_data_iterator(examples_to_display)\n",
        "for pred in tqdm(make_predictions(valid_data_iter,\n",
        "                     postprocess_tokens=paligemma_tokenizer.postprocess_tokens,\n",
        "                     num_examples=16)):\n",
        "  predictions_after_finetune.append(data_processing.DocumentEditingLabel.from_string(pred))\n",
        "\n",
        "correct_class_after_finetune = []\n",
        "\n",
        "for (page_id, example), prediction in zip(examples_to_display, predictions_after_finetune):\n",
        "  if prediction is None:\n",
        "    correct_class_after_finetune.append(False)\n",
        "    continue\n",
        "  correct_class_after_finetune.append(prediction.gesture == example['classname'])\n",
        "\n",
        "print(f'Gesture classification accuracy: {np.mean(correct_class_after_finetune)}')\n",
        "for i, pred in enumerate(predictions_after_finetune):\n",
        "  print('prediction:', pred)\n",
        "  print('target:    ', examples_to_display[i][1]['label'])\n",
        "  print('-----------')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "dgx4QQ_Mq7AT",
        "O8b9DN-q9EQt",
        "k13yOXeu7j0Q",
        "Fd4bHZ-2vNuM",
        "4qcVfxLwvwiu",
        "ju3FvYNlgWas"
      ],
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
